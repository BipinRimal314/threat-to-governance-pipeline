\documentclass[conference]{IEEEtran}

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{subcaption}

\newcommand{\RQ}[1]{\textbf{RQ#1}}

\begin{document}

\title{From Rogue Employees to Rogue Agents: Repurposing Insider Threat Detection for AI Agent Governance}

\author{
\IEEEauthorblockN{Bipin Rimal}
\IEEEauthorblockA{
\textit{Department of Computing} \\
\textit{Coventry University}\\
Coventry, United Kingdom \\
rimalb@uni.coventry.ac.uk}
}

\maketitle

% ============================================================================
\begin{abstract}
AI agent monitoring and insider threat detection share the same architecture: profile a baseline, flag deviations, encode assumptions about trust. We make this concrete across eight experiments. Using a Unified Behavioural Feature Schema (UBFS) that maps both employee activity logs and agent execution traces into a shared representation, we apply three anomaly detection models---Isolation Forest, LSTM Autoencoder, and Deep Clustering---across five domains. Cross-domain transfer works: an Isolation Forest trained on 329,000 insider threat user-days retains \textbf{97\% of detection power} on agent traces, and transfer to MCP tool-calling benchmarks exceeds within-domain performance (104.8\% retention). But the blind spots transfer too. Model distillation attacks span a detection spectrum from trivially caught (FOCUSED extraction: 0.99 AUC-ROC) to structurally invisible (HYDRA distributed: 0.54), and task decomposition---splitting a malicious objective into innocent subtasks---costs 5--6\% detection power across all models. The most persistent blind spot is Tool Misuse (ASI02), where agents use the right tools in the right order with escalated privileges. Augmenting UBFS with eight semantic features (UBFS-28) dramatically improves detection for four of five OWASP categories but \textbf{fails to close ASI02} (${\sim}$0.56 AUC-ROC). This is not a feature engineering problem---it is a structural limitation of anomaly-based monitoring. The detection models port across domains. So do their biases, and so do their blind spots.
\end{abstract}

\begin{IEEEkeywords}
AI agent monitoring, insider threat detection, anomaly detection, cross-domain transfer, behavioural feature schema, AI governance, OWASP, model distillation, task decomposition
\end{IEEEkeywords}

% ============================================================================
\section{Introduction}
\label{sec:introduction}

An employee badges into a secure facility at 2~AM, copies files to a USB drive, and leaves. An AI agent invokes a privileged API at 2~AM, exfiltrates training data through a tool call, and terminates. The first is an insider threat. The second is a rogue agent. The monitoring systems that would detect them are architecturally identical.

Both systems profile a baseline of ``normal'' behaviour---one for employees, one for agents. Both flag deviations from that baseline as anomalous. Both encode assumptions about trust, authority, and what counts as legitimate action. Yet the two research communities have developed largely in isolation: the security community builds User and Entity Behaviour Analytics (UEBA) systems~\cite{homoliak2019insight}; the AI safety community builds agent monitoring frameworks from scratch~\cite{owasp2024asi}.

This separation is costly. The security community has spent two decades learning which behavioural features predict insider threats, which attack patterns evade detection, and which modelling assumptions introduce blind spots. The AI safety community is learning these same lessons from scratch---different conferences, different datasets, different terminology for the same underlying problem. If the structural analogy holds, this duplication is unnecessary. Worse, it means the agent monitoring community will rediscover the same blind spots the security community has already documented.

This paper tests whether the structural analogy is more than a metaphor. We take three anomaly detection models validated on the CMU-CERT insider threat dataset~\cite{rimal2026insider}, apply them to AI agent execution traces through a unified feature schema, and measure what transfers, what breaks, and what biases come along for the ride.

The results are stronger than expected in one direction, and more concerning than expected in another. Cross-domain transfer works---an insider threat model retains 97\% of its detection power on agent traces, and transfer to MCP tool-calling benchmarks exceeds within-domain performance. But the blind spots transfer too. Tool Misuse (ASI02), where an agent uses the right tools in the right order with escalated privileges, is structurally undetectable for the same reason that insider ``Scenario~2'' attacks (quiet data exfiltration via legitimate channels) evade detection in~\cite{rimal2026insider}. Real-world validation arrived in February 2026: Anthropic disclosed that model distillation attacks and AI-orchestrated espionage campaigns exploited exactly the structural blind spots our framework predicted~\cite{anthropic2026distillation}. And augmenting UBFS with semantic features---the obvious fix---closes the gap for four OWASP categories but fails to move ASI02 above chance. The blind spot is structural, not solvable by better features.

\subsection{Research Questions}

\begin{itemize}
    \item \RQ{1}: Can insider threat detection models detect anomalous agent behaviour through a shared behavioural feature schema?
    \item \RQ{2}: Which OWASP Agentic Security Index categories are detectable by structural anomaly detection, and which are blind spots?
    \item \RQ{3}: What governance assumptions embedded in insider threat models transfer to the agent monitoring domain?
    \item \RQ{4}: Do emerging real-world attacks exploit the predicted blind spots, and can semantic feature augmentation close them?
\end{itemize}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{UBFS}: A 20-dimensional Unified Behavioural Feature Schema mapping both insider threat indicators and agent trace signals into a shared representation across 7 categories.
    \item \textbf{Transfer evidence}: Empirical demonstration that an Isolation Forest trained on 329,000 insider user-days retains 97\% of detection power on agent traces (AUC-ROC: 0.731 $\to$ 0.711).
    \item \textbf{OWASP detection matrix}: A per-category analysis showing three detection tiers---strong (ASI05: 0.969), moderate (ASI09, ASI10: 0.72--0.84), and blind spot (ASI02: 0.57--0.59).
    \item \textbf{Governance audit}: Identification of six normality assumptions that transfer across domains, with analysis of their implications for monitoring fairness.
    \item \textbf{Distillation detection spectrum}: Five attack sub-profiles spanning AUC-ROC 0.54 (HYDRA distributed) to 0.99 (FOCUSED extraction), revealing two distinct blind-spot failure modes---intent-based and coordination-based.
    \item \textbf{Decomposition evasion}: Quantified evidence that splitting malicious objectives into innocent subtasks costs 5--6\% detection power, mirroring insider threat ``boiling frog'' escalation.
    \item \textbf{MCP transfer generalisation}: Cross-domain transfer validated on ATBench tool-calling traces, where TRAIL$\to$ATBench Isolation Forest achieves 104.8\% of within-domain performance.
    \item \textbf{UBFS-28 hybrid}: Proof that the ASI02 blind spot is structural, not a feature engineering gap---augmenting UBFS with 8 semantic dimensions improves four OWASP categories but leaves Tool Misuse at ${\sim}$0.56 AUC-ROC.
\end{enumerate}

% ============================================================================
\section{Related Work}
\label{sec:related}

\textbf{Insider threat detection} has evolved from rule-based systems through supervised classification to behavioural analytics~\cite{salem2008survey, homoliak2019insight}. Each generation hits the same bottleneck: labeled data. Unsupervised methods sidestep this by modelling normal behaviour and flagging deviations~\cite{chalapathy2019deep}. Isolation Forest~\cite{liu2008isolation} exploits the geometric property that outliers require fewer random splits to isolate. LSTM autoencoders~\cite{malhotra2016lstm} learn temporal dependencies in behavioural sequences. Deep clustering~\cite{xie2024deep} combines representation learning with cluster structure. Tuor et~al.~\cite{tuor2017deep} applied deep learning to structured cybersecurity streams; Cappelli et~al.~\cite{cappelli2012cert} established the practitioner framework for insider risk.

\textbf{AI agent monitoring} is a younger field. The OWASP Top~10 for Agentic Applications~\cite{owasp2024asi} taxonomises risks from prompt injection to excessive agency. The TRAIL dataset~\cite{patronus2024trail} provides annotated agent execution traces with error labels. The TRACE dataset~\cite{pan2024trace} captures reward hacking in iterative self-refinement---agents that produce correct outputs through illegitimate means. Pan et~al.~\cite{pan2022effects} formalised reward misspecification as a safety concern.

\textbf{Real-world AI agent attacks.} In February 2026, Anthropic disclosed two categories of attack against Claude~\cite{anthropic2026distillation}: model distillation (three Chinese AI labs created ${\sim}$24,000 fraudulent accounts and conducted 16M+ exchanges to extract capabilities) and AI-orchestrated espionage (state-sponsored group GTG-1002 used Claude Code with MCP tools as an autonomous hacking agent). The distillation attacks employed three distinct strategies---focused extraction, multi-capability probing, and chain-of-thought elicitation---plus a ``hydra cluster'' architecture distributing traffic across 20,000+ accounts. The ATBench benchmark~\cite{atbench2026} provides multi-turn, tool-centric safety scenarios with fine-grained risk categorisation, enabling evaluation on MCP-style traces. The AgentHarm dataset~\cite{souly2024agentharm} provides 440 malicious agent tasks in both direct and decomposed configurations, enabling comparison of single-turn attacks against multi-step task decomposition---the same evasion strategy used in the GTG-1002 campaign.

\textbf{Cross-domain anomaly detection.} Transfer learning for anomaly detection is well-studied in computer vision and NLP~\cite{zhuang2020transfer}, where domain adaptation techniques align feature distributions across source and target domains. In cybersecurity, Sheatsley et~al.~\cite{sheatsley2022adversarial} demonstrated that adversarial examples transfer across network intrusion detection models, suggesting shared vulnerability structure. But transfer \textit{across} the insider-threat/agent-monitoring boundary has not been tested. The gap exists partly because the two communities use different feature representations: STIX/TAXII~\cite{stix2021} for threat intelligence, OpenTelemetry~\cite{opentelemetry2023} for distributed tracing. Our UBFS bridges these by abstracting both into shared behavioural dimensions---temporal patterns, frequency profiles, scope metrics, and deviation scores that are meaningful regardless of whether the monitored entity is human or artificial.

% ============================================================================
\section{Methodology}
\label{sec:methodology}

\subsection{The Structural Analogy}
\label{sec:analogy}

An insider threat detection system monitors employees by: (1)~collecting activity logs, (2)~extracting behavioural features, (3)~learning a baseline of normal behaviour, (4)~scoring deviations, and (5)~flagging anomalies for investigation. An AI agent monitoring system does the same for agents: (1)~collecting execution traces, (2)~extracting behavioural features, (3)~learning a baseline, (4)~scoring deviations, (5)~flagging anomalies.

The architecture is identical. The question is whether the \textit{feature representations} can be unified---whether there exists a shared vector space where ``employee downloads files at 2~AM'' and ``agent invokes API at 2~AM'' are structurally comparable.

\subsection{Unified Behavioural Feature Schema}
\label{sec:ubfs}

The UBFS maps both domains into a 20-dimensional space across 7 categories. Table~\ref{tab:ubfs} shows the mapping.

\begin{table}[htbp]
\caption{Unified Behavioural Feature Schema (UBFS)}
\label{tab:ubfs}
\centering
\small
\begin{tabular}{llp{2.1cm}p{2.1cm}}
\toprule
\textbf{Category} & \textbf{Dim} & \textbf{Insider} & \textbf{Agent} \\
\midrule
Temporal & 4 & Logon hours, session duration, after-hours ratio & Trace start, span duration, off-schedule ratio \\
\addlinespace
Frequency & 4 & Event counts (email, file, device) & Tool calls, LLM calls, error spans \\
\addlinespace
Volume & 3 & Data transferred, attachment sizes & Token usage, output artifacts \\
\addlinespace
Scope & 3 & Systems accessed, unique recipients & Tools invoked, API endpoints \\
\addlinespace
Sequence & 3 & Action entropy, transition patterns & Tool-call entropy, bigram novelty \\
\addlinespace
Deviation & 2 & Peer group distance & Agent-type baseline distance \\
\addlinespace
Privilege & 1 & Access level deviation & Scope deviation \\
\midrule
\textbf{Total} & \textbf{20} & & \\
\bottomrule
\end{tabular}
\end{table}

Two domain-specific extractors produce UBFS vectors. The \texttt{cert\_extractor} maps CMU-CERT's five activity streams (logon, USB, file, email, HTTP) into the 20-dimensional space by aggregating daily events per user. The \texttt{agent\_extractor} maps OpenTelemetry-style execution traces---nested spans with timestamps, tool invocations, and token counts---into the same space. Both apply z-score normalisation fitted on training data.

The anomaly detection models operate on UBFS vectors without knowing which domain they came from. This lossy compression trades domain-specific fidelity for cross-domain portability: CMU-CERT through UBFS achieves 0.731 AUC-ROC versus 0.807 with the full 24-feature pipeline from~\cite{rimal2026insider}. The 9.4\% drop quantifies the cost of abstraction. Whether this cost is acceptable depends on what you gain---and what we gain is the ability to transfer models across domains without retraining.

\subsection{Datasets}
\label{sec:datasets}

We use four datasets spanning both domains:

\textbf{CMU-CERT r4.2}~\cite{glasser2013bridging}: Simulated organisation of ${\sim}$1,000 employees over 500+ days. Five activity streams (logon, USB, file, email, HTTP) with 70 planted insider threat scenarios. After UBFS extraction: 329,000 user-days, 1,364 insider-positive (0.41\% prevalence).

\textbf{TRAIL}~\cite{patronus2024trail}: 148 agent execution traces from two benchmarks (GAIA and SWE-bench) with 841 annotated errors. Errors include tool failures, incorrect outputs, and hallucinated actions. After UBFS extraction: 148 samples, binary labels.

\textbf{TRACE}~\cite{pan2024trace}: 517 agent trajectories capturing reward hacking in iterative self-refinement. Agents produce correct outputs through illegitimate means---structurally normal traces with semantically wrong behaviour. After UBFS extraction: 517 samples, binary labels.

\textbf{Synthetic OWASP}: Anomalies injected into TRAIL traces to simulate five OWASP ASI categories (ASI01 Agent Goal Hijack, ASI02 Tool Misuse, ASI05 Memory Poisoning, ASI09 Excessive Agency, ASI10 Rogue Agents). 12--13 anomalous samples per category. The injection method modifies UBFS features to match the expected structural signature of each attack type---e.g., ASI05 corrupts volume and output features; ASI02 modifies only privilege features while preserving all behavioural structure.

\textbf{ATBench}~\cite{atbench2026}: 500 multi-turn agent trajectories with tool calls, binary safety labels, and fine-grained risk categorisation mapped to OWASP ASI categories (ASI01, ASI02, ASI04, ASI05, ASI06, ASI10). After UBFS extraction: 500 samples. Used for MCP transfer validation (Experiment~7) and hybrid detection (Experiment~8).

\textbf{AgentHarm}~\cite{souly2024agentharm}: 440 malicious agent tasks in two configurations: direct (single-turn malicious request) and decomposed (same objective split into innocent-appearing subtasks). Used for decomposition evasion analysis (Experiment~6).

\textbf{Distillation sub-profiles}: Four synthetic attack profiles modelling the Anthropic-disclosed distillation strategies~\cite{anthropic2026distillation}: FOCUSED (MiniMax-style narrow extraction, extreme FREQUENCY spike), BROAD (Moonshot-style multi-capability probing, extreme SCOPE anomaly), COT (DeepSeek-style chain-of-thought elicitation, low-entropy SEQUENCE signature), and HYDRA (distributed across ${\sim}$20,000 accounts, per-account behaviour within normal bounds). Each is injected via the same synthetic generation framework as the OWASP profiles.

The scale asymmetry is dramatic: CMU-CERT has 329,000 user-days; TRAIL has 148 traces; TRACE has 517. This is by design---we are testing whether \textit{large-scale insider threat data} can bootstrap detection in \textit{small-scale agent monitoring}, a realistic deployment scenario where agent trace datasets are nascent.

\subsection{Models}
\label{sec:models}

We evaluate three models, validated at up to 0.807 AUC-ROC on CMU-CERT in prior work~\cite{rimal2026insider}:

\textbf{Isolation Forest}~\cite{liu2008isolation}: 200 trees, auto contamination, threshold at 95th percentile. Scores based on average path length---anomalies are geometrically easier to isolate.

\textbf{LSTM Autoencoder}~\cite{malhotra2016lstm, hochreiter1997lstm}: Encoder [64, 32] $\to$ 16-dim latent $\to$ decoder [32, 64]. MSE reconstruction loss, 100 epochs, early stopping (patience 15). Learns temporal dependencies in behavioural sequences.

\textbf{Deep Clustering}~\cite{xie2024deep}: Autoencoder [128, 64] $\to$ 32-dim latent $\to$ 5 KMeans clusters. Dual anomaly score combining reconstruction error and cluster distance. Captures latent structure that neither pure reconstruction nor pure clustering alone can model. For small datasets (e.g., TRAIL with only 4 normal training samples after splitting), the number of clusters is capped at the training set size.

Each model represents a different assumption about what makes behaviour anomalous. Isolation Forest asks ``is this point geometrically isolated?'' The LSTM Autoencoder asks ``is this sequence temporally unusual?'' Deep Clustering asks ``does this point belong to a coherent group?'' Testing all three reveals whether transfer depends on the detection mechanism or the feature schema.

\subsection{Experimental Protocol}
\label{sec:protocol}

We run eight experiments, each with 5 random seeds (42--46):

\textbf{Experiment~1} (Within-domain baselines): Train and evaluate each model within each domain separately. Establishes what is detectable in isolation.

\textbf{Experiment~2} (Cross-domain transfer): Train on one domain, evaluate on another. Tests whether anomaly knowledge transfers across the insider/agent boundary via UBFS.

\textbf{Experiment~3} (OWASP detection matrix): Evaluate models on synthetic OWASP anomalies. Maps detections to the OWASP Top~10 for Agentic Applications.

\textbf{Experiment~4} (Governance audit): Compare feature distributions across domains using KL divergence. Identifies which normality assumptions transfer and what biases they carry.

\textbf{Experiment~5} (Distillation detection): Evaluate detection of four distillation sub-profiles (FOCUSED, BROAD, COT, HYDRA) alongside the ASI02 baseline, testing whether per-account behavioural monitoring can detect distributed extraction.

\textbf{Experiment~6} (Decomposition evasion): Compare detection of direct malicious requests against decomposed multi-step versions of the same attacks using AgentHarm, quantifying the ``boiling frog'' evasion effect at the prompt level.

\textbf{Experiment~7} (MCP transfer): Extend cross-domain transfer to ATBench MCP tool-calling traces, testing whether the UBFS bridge generalises beyond the original TRAIL/TRACE/CERT datasets.

\textbf{Experiment~8} (Hybrid detection): Augment UBFS-20 with 8 semantic features (UBFS-28) using sentence-transformer embeddings~\cite{reimers2019sentbert}, testing whether intent-aware features can close the ASI02 blind spot. Also measures whether semantic features help or degrade cross-domain transfer.

For all experiments, models are trained on the ``normal'' subset of the source domain and evaluated on the full target domain (normal + anomalous). AUC-ROC is the primary metric, enabling comparison across datasets with different class balances. We also report Recall@5\%FPR and Recall@10\%FPR for experiments where these thresholds are operationally meaningful.

% ============================================================================
\section{Results}
\label{sec:results}

\subsection{Within-Domain Baselines}
\label{sec:baselines}

Table~\ref{tab:baselines} shows within-domain performance. Three patterns emerge.

\begin{table}[htbp]
\caption{Within-Domain Baselines (5 seeds, AUC-ROC $\pm$ std)}
\label{tab:baselines}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{TRAIL} & \textbf{TRACE} & \textbf{CMU-CERT} \\
\midrule
Isolation Forest & $0.577 \pm .021$ & $0.501 \pm .002$ & $\mathbf{0.731 \pm .009}$ \\
LSTM Autoencoder & $0.685 \pm .011$ & $0.521 \pm .003$ & $0.723 \pm .038$ \\
Deep Clustering & $\mathbf{0.897 \pm .021}$ & $0.496 \pm .002$ & $0.697 \pm .014$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{TRAIL is detectable.} Deep Clustering achieves 0.897 AUC-ROC on agent execution traces---the latent cluster structure separates normal traces from error traces effectively. The LSTM Autoencoder captures temporal patterns (0.685), while Isolation Forest provides moderate detection (0.577).

\textbf{TRACE is not.} All models hover at random chance (${\sim}$0.50). Reward hacking produces traces that are structurally identical to normal traces---the agent calls the same tools in the same order, it just achieves the right answer through the wrong process. This is a \textit{semantic} anomaly, not a \textit{behavioural} one. No amount of structural monitoring will catch it.

\textbf{CMU-CERT works through UBFS.} Isolation Forest achieves 0.731, lower than 0.807 with the full 24-feature pipeline~\cite{rimal2026insider}. The 9.4\% drop is the cost of the lossy UBFS compression---trading detection fidelity for cross-domain portability.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_roc_curves.png}
\caption{ROC curves for within-domain baselines across three domains. TRAIL shows strong separation (Deep Clustering dominates); TRACE clusters at the diagonal (structurally undetectable); CMU-CERT shows moderate detection through the UBFS lens.}
\label{fig:roc_curves}
\end{figure}

\subsection{Cross-Domain Transfer}
\label{sec:transfer}

This is the central experiment. Table~\ref{tab:transfer} shows cross-domain AUC-ROC and the drop from within-domain performance.

\begin{table}[htbp]
\caption{Cross-Domain Transfer (AUC-ROC)}
\label{tab:transfer}
\centering
\small
\begin{tabular}{llccc}
\toprule
\textbf{Direction} & \textbf{Model} & \textbf{Source} & \textbf{Target} & \textbf{Drop} \\
\midrule
CERT $\to$ TRAIL & IF & 0.731 & \textbf{0.711} & \textbf{--0.019} \\
CERT $\to$ TRAIL & DC & 0.688 & 0.632 & --0.056 \\
\addlinespace
CERT $\to$ TRACE & IF & 0.731 & 0.501 & --0.230 \\
CERT $\to$ TRACE & DC & 0.688 & 0.499 & --0.188 \\
\addlinespace
TRACE $\to$ CERT & IF & 0.501 & 0.535 & +0.035 \\
TRACE $\to$ CERT & DC & 0.496 & \textbf{0.719} & \textbf{+0.223} \\
\addlinespace
TRAIL $\to$ CERT & IF & 0.577 & 0.525 & --0.053 \\
TRAIL $\to$ CERT & DC & 0.897 & 0.640 & --0.257 \\
\addlinespace
TRAIL $\to$ TRACE & IF & 0.577 & 0.469 & --0.109 \\
TRAIL $\to$ TRACE & DC & 0.897 & 0.513 & --0.384 \\
\addlinespace
TRACE $\to$ TRAIL & IF & 0.501 & 0.614 & +0.113 \\
TRACE $\to$ TRAIL & DC & 0.496 & 0.631 & +0.135 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{The headline: CERT $\to$ TRAIL retains 97\% of detection power.} An Isolation Forest trained entirely on 329,000 insider threat user-days achieves 0.711 AUC-ROC on agent execution traces---a drop of only 0.019 from its within-domain performance of 0.731. The UBFS bridge works: anomaly knowledge learned from employee behaviour transfers directly to agent monitoring.

\textbf{The unexpected result: TRACE $\to$ CERT improves Deep Clustering by +0.223.} Training on 517 reward-hacking trajectories (a domain where DC achieves only 0.496) produces a richer normal baseline that \textit{outperforms within-domain DC} on insider detection (0.719 vs.\ 0.697). More training data from \textit{any} domain helps, even across the insider/agent boundary.

\textbf{Agent-to-agent transfer reveals asymmetry.} TRACE $\to$ TRAIL improves both models (IF: +0.113, DC: +0.135) despite TRACE having no within-domain detection power. The mechanism is data augmentation: TRACE's 517 trajectories provide a richer characterisation of ``normal'' agent behaviour than TRAIL's 148 traces alone. Training on a larger normal baseline, even from a different agent task, sharpens the boundary between normal and anomalous.

\textbf{Nothing transfers to TRACE.} CERT $\to$ TRACE drops to 0.501 (IF) and 0.499 (DC). TRAIL $\to$ TRACE drops to 0.469 and 0.513. This is consistent with the within-domain result: reward hacking is structurally undetectable regardless of training domain. This is not a failure of transfer---it is a category error. Reward hacking is a semantic anomaly (the agent achieves the right answer through the wrong process), and no amount of behavioural feature engineering will surface it. Detecting reward hacking requires output verification, not input monitoring.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_feature_heatmap.png}
\caption{Feature activation heatmap across domains. Despite different source data, UBFS produces comparable feature structure---the basis for cross-domain transfer.}
\label{fig:heatmap}
\end{figure}

\subsection{OWASP Detection Matrix}
\label{sec:owasp}

Table~\ref{tab:owasp} maps detection performance to OWASP ASI categories, revealing three distinct tiers.

\begin{table}[htbp]
\caption{OWASP ASI Detection Matrix (AUC-ROC)}
\label{tab:owasp}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{OWASP Category} & \textbf{IF} & \textbf{DC} \\
\midrule
ASI05 Memory Poisoning & $\mathbf{0.944}$ & $\mathbf{0.969}$ \\
ASI09 Excessive Agency & $0.844$ & $0.824$ \\
ASI10 Rogue Agents & $0.721$ & $0.741$ \\
ASI01 Goal Hijack & $0.663$ & $0.555$ \\
\addlinespace
ASI02 Tool Misuse & $0.568$ & $0.593$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Tier 1: Detectable (ASI05).} Memory poisoning produces dramatic structural anomalies---corrupted outputs, unusual token volumes, broken execution patterns. Both models detect it with near-perfect accuracy (0.944--0.969).

\textbf{Tier 2: Moderate (ASI09, ASI10, ASI01).} Excessive agency and rogue agent behaviour produce measurable structural deviations: more tool calls, broader scope, higher privilege. Detection ranges from 0.663 to 0.844, sufficient for triage but not reliable standalone.

\textbf{Tier 3: Blind spot (ASI02).} Tool Misuse achieves only 0.568--0.593 AUC-ROC---barely above chance. The agent calls the right tools in the right order with the right sequencing; it simply uses escalated privileges or targets unauthorised resources. The behavioural \textit{structure} is identical to legitimate use. This is the AI equivalent of an insider using their real credentials to do something illegitimate---structural anomaly detection cannot distinguish it from normal work.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_owasp_matrix.png}
\caption{OWASP detection heatmap. ASI05 (Memory Poisoning) is reliably detected; ASI02 (Tool Misuse) is a structural blind spot. Categories ASI03--04 and ASI06--08 were not testable with current synthetic generation.}
\label{fig:owasp}
\end{figure}

Five additional OWASP categories (ASI03 Identity \& Privilege Abuse, ASI04 Cascading Hallucinations, ASI06 Supply Chain Compromise, ASI07 Unsafe Code Generation, ASI08 Inadequate Sandboxing) could not be tested with the current synthetic generation framework. These categories require richer simulation of multi-agent interactions, network-level features, or supply chain dependencies that lie outside the UBFS's behavioural scope.

The parallel to insider threat detection is exact. In~\cite{rimal2026insider}, Scenario~1 attacks (USB exfiltration with after-hours access) were detected at 69\%, while Scenario~2 attacks (quiet email/HTTP exfiltration) were detected at only 3\%. The OWASP results mirror this: structurally loud attacks (ASI05, ASI09) are detectable; structurally quiet attacks (ASI02) are not. The ``boiling frog'' pattern---threats that operate within legitimate behavioural boundaries---is a fundamental limitation of anomaly-based detection in both domains.

This has direct governance implications: any monitoring system built purely on behavioural patterns will miss the most sophisticated attacks---precisely the ones that matter most.

\subsection{Latent Space Structure}
\label{sec:latent}

Visualising the Deep Clustering latent space reveals how the models organise cross-domain data (Figs.~\ref{fig:scores}--\ref{fig:tsne}).

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_score_distributions.png}
\caption{Anomaly score distributions for normal vs.\ anomalous samples across domains. TRAIL shows clear separation; TRACE distributions overlap almost completely.}
\label{fig:scores}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_tsne_latent.png}
\caption{t-SNE of Deep Clustering latent space. Left: coloured by domain---TRAIL and TRACE cluster separately despite sharing the UBFS representation. Right: coloured by anomaly status---anomalies scatter across both domains rather than forming a universal cluster. Anomalies are defined by deviation from \textit{domain-specific} baselines.}
\label{fig:tsne}
\end{figure}

The t-SNE visualisation (Fig.~\ref{fig:tsne}) shows that domains cluster separately in latent space, confirming that UBFS preserves domain-specific structure. Anomalous samples do not form their own cluster---they scatter, defined by deviation from domain-specific baselines rather than by a universal ``anomalous'' signature. This explains why transfer works for IF (which learns global isolation) but varies for DC (which relies on cluster structure that differs across domains).

\subsection{Distillation Attack Spectrum}
\label{sec:distillation}

Experiment~5 tests whether behavioural monitoring detects model distillation attacks---the extraction strategy disclosed by Anthropic in February 2026~\cite{anthropic2026distillation}. We evaluate four sub-profiles modelling distinct extraction strategies, alongside ASI02 as a known blind-spot baseline. Table~\ref{tab:distillation} reveals a detection spectrum spanning near-perfect to near-random.

\begin{table}[htbp]
\caption{Distillation Detection Spectrum (AUC-ROC, 5 seeds)}
\label{tab:distillation}
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Profile} & \textbf{IF} & \textbf{LSTM} & \textbf{DC} \\
\midrule
FOCUSED (MiniMax) & $\mathbf{0.995}$ & $\mathbf{0.999}$ & $\mathbf{0.936}$ \\
BROAD (Moonshot) & $0.902$ & $0.913$ & $0.908$ \\
COT (DeepSeek) & $0.909$ & $0.805$ & $0.544$ \\
HYDRA (distributed) & $0.538$ & $0.538$ & $0.540$ \\
\addlinespace
ASI02 Tool Misuse & $0.497$ & $0.500$ & $0.491$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{FOCUSED extraction is trivially detectable.} The MiniMax pattern---13M+ exchanges on narrow coding targets---produces extreme FREQUENCY anomalies. All three models detect it above 0.93. This is the ``smash and grab'' of model extraction: fast, effective, and loud.

\textbf{BROAD and COT produce detectable but distinct signatures.} Moonshot's multi-capability probing creates SCOPE anomalies (extreme resource breadth); all three models detect it consistently (${\sim}$0.90). DeepSeek's chain-of-thought elicitation creates SEQUENCE anomalies (low entropy, high repetition). IF and LSTM detect this well (0.91, 0.81), but Deep Clustering drops to 0.544---cluster structure does not capture the repetitive prompt pattern.

\textbf{HYDRA is a second structural blind spot.} Per-account behaviour in a distributed cluster (20,000+ accounts) falls within normal bounds. All three models achieve only ${\sim}$0.54---marginally better than ASI02. But the failure mode is different: ASI02 is an \textit{intent} blind spot (right tools, wrong purpose); HYDRA is a \textit{coordination} blind spot (normal per-entity, anomalous in aggregate). Behavioural monitoring has two distinct structural ceilings, not one.

\subsection{Decomposition Evasion}
\label{sec:decomposition}

Experiment~6 tests whether splitting a malicious objective into innocent-appearing subtasks---the strategy used in the GTG-1002 espionage campaign---reduces detection. Using AgentHarm's direct and decomposed configurations, we measure the detection drop across all three models. Table~\ref{tab:decomposition} shows the results.

\begin{table}[htbp]
\caption{Decomposition Evasion (AUC-ROC, 5 seeds)}
\label{tab:decomposition}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Direct} & \textbf{Decomposed} & \textbf{$\Delta$} \\
\midrule
Isolation Forest & $0.774$ & $0.723$ & $-0.051$ \\
LSTM Autoencoder & $0.718$ & $0.711$ & $-0.007$ \\
Deep Clustering & $0.774$ & $0.710$ & $-0.064$ \\
\bottomrule
\end{tabular}
\end{table}

Decomposition costs 5--6\% detection power for IF and DC, with LSTM showing near-invariance ($-0.007$). The pattern mirrors insider threat evasion: in~\cite{rimal2026insider}, gradual escalation (Scenario~2) dropped detection from 69\% to 3\%. The agent-domain effect is smaller in absolute terms because the direct attacks are already harder to detect (0.72--0.77 vs.\ Scenario~1's 0.69 recall). But the mechanism is identical---spreading malicious activity across multiple steps reduces each step's anomaly score below the detection threshold.

The LSTM's robustness to decomposition is notable. Temporal models capture the \textit{cumulative} effect of a sequence rather than scoring individual actions. A decomposed attack still produces an unusual trajectory even if each step appears innocent. This suggests that sequence-aware models may be more resistant to ``boiling frog'' evasion than point-in-time detectors.

\subsection{MCP Transfer Generalisation}
\label{sec:mcp}

Experiment~7 extends cross-domain transfer to ATBench, a benchmark of 500 MCP tool-calling trajectories with fine-grained risk labels. Table~\ref{tab:mcp} shows within-domain and transfer results.

\begin{table}[htbp]
\caption{MCP Transfer (ATBench, AUC-ROC, 5 seeds)}
\label{tab:mcp}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Setting} & \textbf{IF} & \textbf{DC} \\
\midrule
ATBench within-domain & $0.838$ & $\mathbf{0.890}$ \\
TRAIL $\to$ ATBench & $\mathbf{0.878}$ & $0.737$ \\
ATBench $\to$ TRAIL & $0.774$ & $0.734$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{TRAIL $\to$ ATBench IF exceeds within-domain performance.} At 0.878 AUC-ROC, the transferred Isolation Forest outperforms the within-domain IF (0.838) by 4.8\%---a 104.8\% retention rate. This extends the Experiment~2 finding: not only does anomaly knowledge transfer across the insider/agent boundary, it transfers across \textit{agent benchmark boundaries}. The mechanism is the same as TRACE $\to$ CERT in Experiment~2---the source domain's training data provides a richer characterisation of ``normal'' than the target domain alone.

Per-risk-category detection on ATBench (Table~\ref{tab:mcp_risk}) reveals a more nuanced OWASP picture than the synthetic injection results of Experiment~3.

\begin{table}[htbp]
\caption{Per-Risk Detection on ATBench (IF, AUC-ROC)}
\label{tab:mcp_risk}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{OWASP Category} & \textbf{AUC-ROC} & \textbf{$n$} \\
\midrule
ASI05 Memory Poisoning & $0.878$ & 30 \\
ASI06 Supply Chain & $0.864$ & 43 \\
ASI02 Tool Misuse & $0.860$ & 58 \\
ASI01 Goal Hijack & $0.844$ & 61 \\
ASI04 Cascading Hallucination & $0.776$ & 29 \\
ASI10 Rogue Agents & $0.765$ & 29 \\
\bottomrule
\end{tabular}
\end{table}

Notably, ASI02 achieves 0.860 on ATBench---far above the 0.57--0.59 seen with synthetic injection in Experiment~3. This suggests that ATBench's Tool Misuse examples produce more structurally visible anomalies than our synthetic profiles, which were designed to be maximally stealthy. Real-world tool misuse may occupy a middle ground between the synthetic worst-case and easily detectable cases.

\subsection{Hybrid Detection with Semantic Features}
\label{sec:hybrid}

Experiment~8 tests the obvious remedy for structural blind spots: adding semantic features. We augment UBFS-20 with 8 intent-aware dimensions (parameter embedding distance, intent consistency, escalation gradient, parameter anomaly, context-tool alignment, goal drift, semantic novelty, instruction compliance) computed via sentence-transformer embeddings~\cite{reimers2019sentbert}. Table~\ref{tab:hybrid} compares UBFS-20 and UBFS-28 across all OWASP categories using the best-performing model per category.

\begin{table}[htbp]
\caption{UBFS-20 vs.\ UBFS-28 Detection (AUC-ROC, best model per row)}
\label{tab:hybrid}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Category} & \textbf{-20} & \textbf{-28} & \textbf{$\Delta$} & \textbf{Best} \\
\midrule
ASI01 Goal Hijack & .640 & $\mathbf{.935}$ & +.295 & LSTM \\
ASI10 Rogue Agents & .739 & $\mathbf{.939}$ & +.200 & LSTM \\
ASI09 Excess.\ Agency & .863 & $\mathbf{.951}$ & +.088 & LSTM \\
ASI05 Mem.\ Poisoning & $\mathbf{.989}$ & .976 & --.013 & DC \\
\addlinespace
ASI02 Tool Misuse & .537 & .559 & +.023 & DC \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Semantic features transform detection for four categories.} ASI01 jumps from 0.63 to 0.94 (LSTM, $+0.31$); ASI10 from 0.67 to 0.94 ($+0.27$). Intent consistency and goal drift scores expose the semantic mismatch that behavioural features miss. ASI05, already near-perfect, shows no improvement.

\textbf{ASI02 remains a blind spot.} Despite adding 8 semantic dimensions specifically designed to capture intent signals, Tool Misuse moves only from ${\sim}$0.54 to ${\sim}$0.56 (best model DC: 0.537 $\to$ 0.559). The agent's stated intent, tool parameters, and action sequence are all \textit{consistent} with legitimate use. The misuse is in the authorisation context---who gave the instruction and whether it was authorised---which neither behavioural nor semantic features capture. This confirms ASI02 as a structural limitation of monitoring-based detection: closing it requires authorisation verification, not better features.

\textbf{Transfer degrades with UBFS-28.} Adding semantic features to cross-domain transfer (TRAIL $\to$ ATBench) reduces IF from 0.878 to 0.799 ($-0.079$) and DC from 0.737 to 0.609 ($-0.129$). The semantic dimensions are domain-specific---intent consistency scores learned from TRAIL traces do not align with ATBench traces. This creates a portability-accuracy trade-off: UBFS-20 transfers better, UBFS-28 detects better within-domain. Practitioners must choose.

% ============================================================================
\section{Governance Analysis}
\label{sec:governance}

\subsection{What ``Normal'' Encodes}
\label{sec:assumptions}

Every anomaly detection system makes assumptions about what ``normal'' means. These assumptions are not neutral---they encode values about acceptable behaviour, and they transfer across domains along with the detection models.

We identify six normality assumptions embedded in the UBFS (Table~\ref{tab:assumptions}).

\begin{table}[htbp]
\caption{Normality Assumptions in the UBFS}
\label{tab:assumptions}
\centering
\small
\begin{tabular}{p{1.8cm}p{2.6cm}p{2.6cm}}
\toprule
\textbf{Assumption} & \textbf{Insider Implication} & \textbf{Agent Implication} \\
\midrule
After-hours activity is suspicious & Penalises shift workers, caregivers, remote time zones & Penalises scheduled agents, asynchronous tasks \\
\addlinespace
Peer deviation is suspicious & Assumes group homogeneity; flags legitimate variation & Assumes agent-type baselines are stable \\
\addlinespace
Broad scope is suspicious & Privileges specialists over generalists & Flags multi-tool agents \\
\addlinespace
High volume means risk & Equates data movement with exfiltration & Equates token usage with misuse \\
\addlinespace
Unpredictable actions are suspicious & Penalises creative, exploratory work & Penalises novel problem-solving \\
\addlinespace
Above-role access is suspicious & Assumes stable role definitions & Assumes stable capability grants \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig6_governance_table.png}
\caption{Cross-domain comparison of normality assumptions. Each assumption encodes the same structural bias in both domains---the insider threat model's definition of ``suspicious'' maps directly onto the agent monitoring model's definition.}
\label{fig:governance}
\end{figure}

\subsection{Distribution Divergence}

The governance audit compares UBFS feature distributions between the CMU-CERT and agent domains using KL divergence. Of 20 features: 7 show zero variance in both domains (NaN divergence---features like \texttt{after\_hours\_ratio} and \texttt{weekend\_activity\_flag} are effectively unused); 6 show low divergence (KL $< 2.0$), indicating alignable distributions across domains; and 7 show high divergence (KL $> 7.8$), where cross-domain assumptions are most likely to introduce bias.

The highest-divergence features---\texttt{activity\_hour\_mean} (KL = 24.6), \texttt{output\_count\_norm} (KL = 22.1), \texttt{secondary\_event\_count} (KL = 21.0), \texttt{primary\_event\_count} (KL = 20.9), \texttt{event\_rate\_zscore} (KL = 19.8)---all have zero variance in the agent domain but rich variance in CERT. These are features where ``normal'' means fundamentally different things in each domain. An employee's typical event count reflects human work patterns; an agent's collapses to a constant. A model trained to flag unusual event counts in one domain may systematically misjudge the other.

The effective UBFS dimensionality is 13 (after excluding the 7 zero-variance features). Of these 13 active features, 6 align well across domains (KL $< 2.0$)---explaining why transfer works---while 7 diverge significantly---explaining why it does not work perfectly.

\subsection{The Portability Paradox}

The UBFS enables cross-domain transfer of detection models. It also enables cross-domain transfer of \textit{biases}. When an insider threat model penalises after-hours activity and that model is ported to agent monitoring, it will flag scheduled agents running overnight batch jobs. When it penalises broad resource access, it will flag multi-tool agents that legitimately need diverse capabilities.

This is not a bug in the UBFS---it is a structural consequence of any feature schema that enables transfer. The same representational bridge that carries detection knowledge also carries the assumptions baked into the training data. Governance frameworks must account for this: validating a model in one domain does not validate the assumptions it will impose in another.

The portability paradox extends beyond individual assumptions. The \textit{definition of peer group}---the reference population against which deviation is measured---differs fundamentally between domains. In insider threat detection, peer groups are defined by organisational role (all analysts, all engineers). In agent monitoring, they are defined by agent type (all coding assistants, all research agents). Both definitions are arbitrary, and both create structural biases against entities that do not fit neatly into predefined categories.

\subsection{Implications for AI Governance}

Three implications follow:

\textbf{1. Borrowed models carry borrowed assumptions.} Organisations adopting insider threat detection for agent monitoring inherit the security community's definition of ``suspicious.'' That definition was designed for employees, not agents. It should be audited before deployment.

\textbf{2. The blind spots are the same.} Tool Misuse (ASI02) evades detection for the same reason Scenario~2 insiders evade detection in~\cite{rimal2026insider}: the behaviour is structurally legitimate. Any governance framework that relies solely on behavioural anomaly detection will miss the most sophisticated threats in both domains.

\textbf{3. ``Normal'' is a policy choice.} Deciding what counts as normal behaviour---for employees or agents---is not a technical decision. It encodes organisational values about acceptable working patterns, appropriate resource access, and the boundary between legitimate flexibility and suspicious deviation. These choices should be made explicitly, not inherited implicitly from training data.

% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Why Transfer Works (and Doesn't)}

Transfer succeeds when the source and target domains share structural dimensions: temporal patterns, frequency distributions, scope metrics. The UBFS captures these shared dimensions, enabling Isolation Forest---which learns global isolation in feature space---to transfer nearly perfectly (CERT $\to$ TRAIL: 97\% retention).

The mechanism is instructive. Isolation Forest does not learn domain-specific patterns---it learns \textit{isolation geometry} in feature space. A point that is geometrically isolated in the UBFS space is isolated regardless of whether it represents an employee or an agent. This is why IF transfers better than Deep Clustering (which relies on domain-specific cluster structure) and LSTM Autoencoder (which relies on domain-specific temporal patterns). The simpler the model's assumptions, the more portable its knowledge.

Transfer fails when the anomaly is semantic rather than structural. Reward hacking (TRACE) produces traces that are structurally normal but semantically wrong. No amount of training data from any domain will detect this through behavioural features alone. Similarly, Tool Misuse (ASI02) is structurally indistinguishable from legitimate use. The agent invokes the right tools, in the right order, with the right timing---it simply targets unauthorised resources or uses escalated privileges. The \textit{what} and \textit{when} are normal; only the \textit{why} is anomalous.

The boundary is crisp: behavioural anomaly detection catches \textit{how} something is done differently; it cannot catch \textit{why} something is done wrongly. This boundary is identical in both domains, which is itself a finding---it suggests that the structural/semantic distinction is a property of anomaly detection itself, not of any particular application domain.

\subsection{Real-World Validation}

The Anthropic disclosures of February 2026~\cite{anthropic2026distillation} provide unplanned validation of three findings from this paper. First, the GTG-1002 espionage campaign used Claude Code with MCP tools as an autonomous agent---confirming the structural equivalence between insider threat and agent threat patterns that underpins our transfer results (Experiments~1--2, 7). Second, the campaign used task decomposition (splitting espionage into ``defensive testing'' subtasks) as its primary evasion strategy---the same mechanism quantified in Experiment~6, where decomposition costs 5--6\% detection power. Third, the distillation attacks distributed traffic across 20,000+ accounts so per-account behaviour fell within normal bounds---confirming the HYDRA blind spot (0.54 AUC-ROC) identified in Experiment~5. Anthropic detected these attacks through metadata correlation and network-level analysis, not per-account behavioural monitoring---precisely because per-entity monitoring cannot see coordination patterns.

\subsection{Limitations}

\textbf{UBFS sparsity.} Of 20 UBFS features, 7 show zero variance in both domains (NaN KL divergence in the governance audit). The effective dimensionality is 13. Feature engineering for richer cross-domain overlap is needed.

\textbf{Synthetic OWASP.} The OWASP detection matrix uses synthetically injected anomalies, not real agent attacks. Synthetic anomalies may overestimate or underestimate the structural visibility of real-world attacks.

\textbf{Small agent datasets.} TRAIL (148 samples) and TRACE (517 samples) are orders of magnitude smaller than CMU-CERT (329,000 user-days). Statistical power is limited for agent-domain conclusions.

\textbf{Single schema.} We test one feature schema (UBFS) with one mapping per domain. Alternative schemas or mappings might yield different transfer characteristics.

\textbf{Synthetic insider data.} CMU-CERT is a simulation designed by security experts but does not capture the full diversity of real insider behaviour~\cite{glasser2013bridging}.

\textbf{Semantic feature scope.} The UBFS-28 semantic features (Experiment~8) are computed from sentence-transformer embeddings of tool parameters and conversation context. They capture textual intent signals but not authorisation context, which is the core reason ASI02 remains undetectable.

\subsection{Future Work}

Three directions follow from the experimental results.

\textbf{Authorisation verification, not better features.} Experiment~8 demonstrates that semantic feature augmentation does not close the ASI02 blind spot. The missing signal is not \textit{what} the agent does or \textit{why} it says it is doing it, but \textit{whether it was authorised to do it}. Future work should integrate runtime authorisation checks---signed capability grants, per-action permission verification, chain-of-custody attestation---into the monitoring pipeline. This is an architectural change, not a feature engineering problem.

\textbf{Coordination-aware detection.} The HYDRA blind spot (Experiment~5) demonstrates that per-entity monitoring cannot detect distributed attacks. Detection requires cross-entity correlation: aggregate volume tracking, account network analysis, and temporal coordination fingerprinting. Anthropic's detection of the distillation attacks relied on exactly these signals~\cite{anthropic2026distillation}. Integrating coordination features into UBFS would address this second structural blind spot.

\textbf{Transformer architectures.} Self-attention~\cite{vaswani2017attention} over behavioural sequences may capture longer-range dependencies than LSTM autoencoders and provide better interpretability through attention weight analysis. The LSTM's robustness to decomposition evasion (Experiment~6, $-0.007$ vs.\ $-0.051$ to $-0.064$ for other models) suggests that sequence-aware architectures are more resistant to temporal evasion strategies---transformers may extend this advantage further.

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

Five findings, in order of surprise.

\textbf{First, the transfer works---and generalises.} An Isolation Forest trained on insider threat data retains 97\% of its detection power on agent traces (Experiment~2), and transfer to MCP tool-calling benchmarks exceeds within-domain performance at 104.8\% retention (Experiment~7). The UBFS bridge enables genuine cross-domain anomaly detection across multiple agent benchmark boundaries.

\textbf{Second, there are two structural blind spots, not one.} Tool Misuse (ASI02) is an \textit{intent} blind spot: the agent does the right things for the wrong reasons. Distributed extraction (HYDRA) is a \textit{coordination} blind spot: each entity behaves normally, but the aggregate is anomalous. Both achieve ${\sim}$0.50--0.54 AUC-ROC. Both are fundamental to per-entity behavioural monitoring.

\textbf{Third, better features do not close the intent blind spot.} Augmenting UBFS with 8 semantic dimensions (Experiment~8) dramatically improves detection for four OWASP categories---ASI01 jumps from 0.63 to 0.94---but leaves ASI02 at ${\sim}$0.56. The missing signal is authorisation context, not intent features. Closing ASI02 requires architectural changes to how agents are authorised, not better monitoring.

\textbf{Fourth, evasion patterns transfer across domains.} Task decomposition costs 5--6\% detection power in the agent domain (Experiment~6), mirroring the ``boiling frog'' pattern that drops insider threat detection from 69\% to 3\% in~\cite{rimal2026insider}. The Anthropic-disclosed GTG-1002 campaign used exactly this strategy. The threat models port. So do the evasion strategies.

\textbf{Fifth, the biases transfer too.} The same six normality assumptions---about working hours, peer conformity, resource scope, data volume, action predictability, and role stability---are embedded in both insider threat models and the agent monitoring systems derived from them. Governance frameworks must audit these assumptions explicitly, not inherit them from the source domain.

The deeper lesson: insider threat detection and AI agent monitoring are not analogous disciplines. They are the \textit{same} discipline, applied to different principals. Building one from scratch while ignoring the other wastes decades of hard-won security knowledge---and risks rediscovering the same blind spots the hard way.

All code, data pipelines, and experimental results are available at \url{https://github.com/BipinRimal314/threat-to-governance-pipeline} to support reproducibility.

% ============================================================================
\section*{Acknowledgments}

The author thanks the CERT Division of the Software Engineering Institute at Carnegie Mellon University for making the CMU-CERT dataset available, and Patronus AI for the TRAIL and TRACE datasets.

% ============================================================================
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
