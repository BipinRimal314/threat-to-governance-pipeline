\documentclass[conference]{IEEEtran}

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{subcaption}

\newcommand{\RQ}[1]{\textbf{RQ#1}}

\begin{document}

\title{From Rogue Employees to Rogue Agents: Repurposing Insider Threat Detection for AI Agent Governance}

\author{
\IEEEauthorblockN{Bipin Rimal}
\IEEEauthorblockA{
Independent Researcher \\
Kathmandu, Nepal \\
bipinrimal314@gmail.com}
}

\maketitle

% ============================================================================
\begin{abstract}
AI agent monitoring and insider threat detection share the same architecture: profile a baseline, flag deviations, encode assumptions about trust. We make this concrete across thirteen experiments. Using a Unified Behavioural Feature Schema (UBFS) that maps both employee activity logs and agent execution traces into a shared representation, we apply three anomaly detection models---Isolation Forest, LSTM Autoencoder, and Deep Clustering---across five domains. Cross-domain transfer works: an Isolation Forest trained on 329,000 insider threat user-days retains \textbf{97\% of detection power} on agent traces, and transfer to MCP tool-calling benchmarks exceeds within-domain performance (104.8\% retention). But the blind spots transfer too. Model distillation attacks span a detection spectrum from trivially caught (FOCUSED extraction: 0.99 AUC-ROC) to near-chance (HYDRA distributed: 0.54), and task decomposition---splitting a malicious objective into innocent subtasks---costs 5--6\% detection power for IF and DC ($p{=}0.031$, Wilcoxon signed-rank), while LSTM Autoencoder shows near-invariance ($-0.7\%$, $p{=}0.094$). Synthetic OWASP profiling identifies Tool Misuse (ASI02) as a blind spot (${\sim}$0.52 AUC-ROC), but \textbf{real-data validation on 500 ATBench trajectories} reveals that this blind spot is an artifact of circular synthetic methodology: real ASI02 achieves 0.81--0.94 AUC-ROC across all models and feature sets. Augmenting UBFS with eight semantic features (UBFS-28) improves detection by up to 13\% on real data. Distillation sensitivity analysis shows detection degrades monotonically with attack intensity for FOCUSED/BROAD/COT profiles, while HYDRA remains near-chance regardless of scale. Adversarial evasion testing reveals that mimicry attacks degrade detection by up to 25\%, with Memory Poisoning (ASI05) most vulnerable. Temporal window ablation finds that 10 spans are optimal for Isolation Forest on both TRAIL and ATBench. Mapping five MITRE ATLAS techniques~\cite{mitre2024atlas} into the UBFS confirms that the framework generalises beyond OWASP: all five techniques are strongly detectable ($>$0.93 AUC-ROC for IF/LSTM), introducing no new blind spots. The detection models port across domains. So do their biases, and so do their blind spots.
\end{abstract}

\begin{IEEEkeywords}
AI agent monitoring, insider threat detection, anomaly detection, cross-domain transfer, behavioural feature schema, AI governance, OWASP, MITRE ATLAS, adversarial evasion, model distillation
\end{IEEEkeywords}

% ============================================================================
\section{Introduction}
\label{sec:introduction}

An employee badges into a secure facility at 2~AM, copies files to a USB drive, and leaves. An AI agent invokes a privileged API at 2~AM, exfiltrates training data through a tool call, and terminates. The first is an insider threat. The second is a rogue agent. The monitoring systems that would detect them are architecturally identical.

Both systems profile a baseline of ``normal'' behaviour---one for employees, one for agents. Both flag deviations from that baseline as anomalous. Both encode assumptions about trust, authority, and what counts as legitimate action. Yet the two research communities have developed largely in isolation: the security community builds User and Entity Behaviour Analytics (UEBA) systems~\cite{homoliak2019insight}; the AI safety community builds agent monitoring frameworks from scratch~\cite{owasp2024asi}.

This separation is costly. The security community has spent two decades learning which behavioural features predict insider threats, which attack patterns evade detection, and which modelling assumptions introduce blind spots. The AI safety community is learning these same lessons from scratch---different conferences, different datasets, different terminology for the same underlying problem. If the structural analogy holds, this duplication is unnecessary. Worse, it means the agent monitoring community will rediscover the same blind spots the security community has already documented.

This paper tests whether the structural analogy is more than a metaphor. We take three anomaly detection models validated on the CMU-CERT insider threat dataset~\cite{rimal2026insider}, apply them to AI agent execution traces through a unified feature schema, and measure what transfers, what breaks, and what biases come along for the ride.

The results are stronger than expected in one direction, and more concerning than expected in another. Cross-domain transfer works---an insider threat model retains 97\% of its detection power on agent traces, and transfer to MCP tool-calling benchmarks exceeds within-domain performance. But the blind spots transfer too. Tool Misuse (ASI02), where an agent uses the right tools in the right order with escalated privileges, is structurally undetectable for the same reason that insider ``Scenario~2'' attacks (quiet data exfiltration via legitimate channels) evade detection in~\cite{rimal2026insider}. Real-world validation arrived in February 2026: Anthropic disclosed that model distillation attacks and AI-orchestrated espionage campaigns exploited exactly the structural blind spots our framework predicted~\cite{anthropic2026distillation}. And augmenting UBFS with semantic features---the obvious fix---closes the gap for four OWASP categories but fails to move synthetic ASI02 above chance, though real-data validation reveals this blind spot is partially an artifact of circular synthetic methodology.

\subsection{Research Questions}

\begin{itemize}
    \item \RQ{1}: Can insider threat detection models detect anomalous agent behaviour through a shared behavioural feature schema?
    \item \RQ{2}: Which OWASP Agentic Security Index categories are detectable by structural anomaly detection, and which are blind spots?
    \item \RQ{3}: What governance assumptions embedded in insider threat models transfer to the agent monitoring domain?
    \item \RQ{4}: Do emerging real-world attacks exploit the predicted blind spots, and can semantic feature augmentation close them?
\end{itemize}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{UBFS}: A 20-dimensional Unified Behavioural Feature Schema mapping both insider threat indicators and agent trace signals into a shared representation across 7 categories.
    \item \textbf{Transfer evidence}: Empirical demonstration that an Isolation Forest trained on 329,000 insider user-days retains 97\% of detection power on agent traces (AUC-ROC: 0.731 $\to$ 0.711).
    \item \textbf{OWASP detection matrix}: A per-category analysis showing three detection tiers---strong (ASI05: 0.975), moderate (ASI09, ASI10: 0.70--0.86), and blind spot (ASI02: 0.51--0.52).
    \item \textbf{Governance audit}: Identification of six normality assumptions that transfer across domains, with analysis of their implications for monitoring fairness.
    \item \textbf{Distillation detection spectrum}: Four distillation sub-profiles spanning AUC-ROC 0.54 (HYDRA distributed) to 0.99 (FOCUSED extraction), benchmarked against ASI02 (0.50), revealing two distinct blind-spot failure modes---intent-based and coordination-based.
    \item \textbf{Decomposition evasion}: Quantified evidence that splitting malicious objectives into innocent subtasks costs 5--6\% detection power for IF and DC ($p{=}0.031$), while LSTM shows near-invariance ($-0.7\%$, $p{=}0.094$), mirroring insider threat ``boiling frog'' escalation.
    \item \textbf{MCP transfer generalisation}: Cross-domain transfer validated on ATBench tool-calling traces, where TRAIL$\to$ATBench Isolation Forest achieves 104.8\% of within-domain performance.
    \item \textbf{UBFS-28 hybrid}: Semantic augmentation improves four OWASP categories on synthetic data but leaves synthetic ASI02 at ${\sim}$0.56 AUC-ROC; however, real-data validation (Experiment~12) shows the synthetic blind spot is an artifact.
    \item \textbf{Real-data validation}: 500 ATBench trajectories reveal that synthetic OWASP profiling dramatically underestimates real detection performance (ASI02: 0.52 synthetic vs.\ 0.81--0.94 real), and UBFS-28 improves most categories by up to 13\% on real data.
    \item \textbf{Distillation sensitivity}: Feature-space scaling across 4 intensity levels shows monotonic detection degradation for FOCUSED/BROAD/COT profiles, while HYDRA remains near-chance (${\sim}$0.52) at original intensity regardless of scale.
    \item \textbf{Adversarial evasion framework}: Three evasion strategies---feature clamping, gradual escalation, and mimicry---quantifying the robustness of UBFS-based detection. Mimicry is the most effective, degrading ASI05 detection by up to 25\%.
    \item \textbf{Temporal window transfer}: Window ablation showing 10 spans optimal for Isolation Forest on both TRAIL and ATBench, paralleling Paper~1's 7-day optimal window---a structural insight about the primacy of early warning signals.
    \item \textbf{MITRE ATLAS mapping}: Five ATLAS adversarial ML techniques mapped to UBFS perturbation profiles, all landing in the ``strong'' detection tier ($>$0.93 AUC-ROC for IF/LSTM)---no new blind spots beyond OWASP ASI02.
\end{enumerate}

% ============================================================================
\section{Related Work}
\label{sec:related}

\textbf{Insider threat detection} has evolved from rule-based systems through supervised classification to behavioural analytics~\cite{salem2008survey, homoliak2019insight}. Each generation hits the same bottleneck: labeled data. Unsupervised methods sidestep this by modelling normal behaviour and flagging deviations~\cite{chalapathy2019deep}. Isolation Forest~\cite{liu2008isolation} exploits the geometric property that outliers require fewer random splits to isolate. LSTM autoencoders~\cite{malhotra2016lstm} learn temporal dependencies in behavioural sequences. Deep clustering~\cite{xie2024deep} combines representation learning with cluster structure. Tuor et~al.~\cite{tuor2017deep} applied deep learning to structured cybersecurity streams; Cappelli et~al.~\cite{cappelli2012cert} established the practitioner framework for insider risk.

\textbf{AI agent monitoring} is a younger field. The OWASP Top~10 for Agentic Applications~\cite{owasp2024asi} taxonomises risks from prompt injection to excessive agency. The TRAIL dataset~\cite{patronus2024trail} provides annotated agent execution traces with error labels. The TRACE dataset~\cite{pan2024trace} captures reward hacking in iterative self-refinement---agents that produce correct outputs through illegitimate means. Pan et~al.~\cite{pan2022effects} formalised reward misspecification as a safety concern.

\textbf{Real-world AI agent attacks.} In February 2026, Anthropic disclosed two categories of attack against Claude~\cite{anthropic2026distillation}: model distillation (three Chinese AI labs created ${\sim}$24,000 fraudulent accounts and conducted 16M+ exchanges to extract capabilities) and AI-orchestrated espionage (state-sponsored group GTG-1002 used Claude Code with MCP tools as an autonomous hacking agent). The distillation attacks employed three distinct strategies---focused extraction, multi-capability probing, and chain-of-thought elicitation---plus a ``hydra cluster'' architecture distributing traffic across 20,000+ accounts. The ATBench benchmark~\cite{atbench2026} provides multi-turn, tool-centric safety scenarios with fine-grained risk categorisation, enabling evaluation on MCP-style traces. The AgentHarm dataset~\cite{andriushchenko2024agentharm} provides 440 malicious agent tasks in both direct and decomposed configurations, enabling comparison of single-turn attacks against multi-step task decomposition---the same evasion strategy used in the GTG-1002 campaign.

\textbf{Adversarial ML threat taxonomies.} MITRE ATLAS~\cite{mitre2024atlas} extends the ATT\&CK framework to adversarial machine learning, cataloguing techniques from model extraction (AML.T0044, AML.T0048) and adversarial data crafting (AML.T0043) to inference API exfiltration (AML.T0025). Where OWASP ASI taxonomises \textit{agent-side} risks, ATLAS taxonomises \textit{model-side} attacks. Testing whether UBFS detects across both taxonomies determines whether the framework generalises to the broader adversarial ML landscape.

\textbf{Cross-domain anomaly detection.} Transfer learning for anomaly detection is well-studied in computer vision and NLP~\cite{zhuang2020transfer}, where domain adaptation techniques align feature distributions across source and target domains. In cybersecurity, Sheatsley et~al.~\cite{sheatsley2022adversarial} demonstrated that adversarial examples transfer across network intrusion detection models, suggesting shared vulnerability structure. But transfer \textit{across} the insider-threat/agent-monitoring boundary has not been tested. The gap exists partly because the two communities use different feature representations: STIX/TAXII~\cite{stix2021} for threat intelligence, OpenTelemetry~\cite{opentelemetry2023} for distributed tracing. Our UBFS bridges these by abstracting both into shared behavioural dimensions---temporal patterns, frequency profiles, scope metrics, and deviation scores that are meaningful regardless of whether the monitored entity is human or artificial.

% ============================================================================
\section{Methodology}
\label{sec:methodology}

\subsection{The Structural Analogy}
\label{sec:analogy}

An insider threat detection system monitors employees by: (1)~collecting activity logs, (2)~extracting behavioural features, (3)~learning a baseline of normal behaviour, (4)~scoring deviations, and (5)~flagging anomalies for investigation. An AI agent monitoring system does the same for agents: (1)~collecting execution traces, (2)~extracting behavioural features, (3)~learning a baseline, (4)~scoring deviations, (5)~flagging anomalies.

The architecture is identical. The question is whether the \textit{feature representations} can be unified---whether there exists a shared vector space where ``employee downloads files at 2~AM'' and ``agent invokes API at 2~AM'' are structurally comparable.

\subsection{Unified Behavioural Feature Schema}
\label{sec:ubfs}

The UBFS maps both domains into a 20-dimensional space across 7 categories. Table~\ref{tab:ubfs} shows the mapping.

\begin{table}[htbp]
\caption{Unified Behavioural Feature Schema (UBFS)}
\label{tab:ubfs}
\centering
\small
\begin{tabular}{llp{2.1cm}p{2.1cm}}
\toprule
\textbf{Category} & \textbf{Dim} & \textbf{Insider} & \textbf{Agent} \\
\midrule
Temporal & 4 & Logon hours, session duration, after-hours ratio & Trace start, span duration, off-schedule ratio \\
\addlinespace
Frequency & 4 & Event counts (email, file, device) & Tool calls, LLM calls, error spans \\
\addlinespace
Volume & 3 & Data transferred, attachment sizes & Token usage, output artifacts \\
\addlinespace
Scope & 3 & Systems accessed, unique recipients & Tools invoked, API endpoints \\
\addlinespace
Sequence & 3 & Action entropy, transition patterns & Tool-call entropy, bigram novelty \\
\addlinespace
Deviation & 2 & Peer group distance & Agent-type baseline distance \\
\addlinespace
Privilege & 1 & Access level deviation & Scope deviation \\
\midrule
\textbf{Total} & \textbf{20} & & \\
\bottomrule
\end{tabular}
\end{table}

Two domain-specific extractors produce UBFS vectors. The \texttt{cert\_extractor} maps CMU-CERT's five activity streams (logon, USB, file, email, HTTP) into the 20-dimensional space by aggregating daily events per user. The \texttt{agent\_extractor} maps OpenTelemetry-style execution traces---nested spans with timestamps, tool invocations, and token counts---into the same space. Both apply z-score normalisation fitted on training data.

The anomaly detection models operate on UBFS vectors without knowing which domain they came from. This lossy compression trades domain-specific fidelity for cross-domain portability: CMU-CERT through UBFS achieves 0.731 AUC-ROC versus 0.807 with the full 24-feature pipeline from~\cite{rimal2026insider}. The 9.4\% drop quantifies the cost of abstraction. Whether this cost is acceptable depends on what you gain---and what we gain is the ability to transfer models across domains without retraining.

\subsection{Datasets}
\label{sec:datasets}

We use four datasets spanning both domains:

\textbf{CMU-CERT r4.2}~\cite{glasser2013bridging}: Simulated organisation of ${\sim}$1,000 employees over 500+ days. Five activity streams (logon, USB, file, email, HTTP) with 70 planted insider threat scenarios. After UBFS extraction: 329,000 user-days, 1,364 insider-positive (0.41\% prevalence).

\textbf{TRAIL}~\cite{patronus2024trail}: 148 agent execution traces from two benchmarks (GAIA and SWE-bench) with 841 annotated errors. Errors include tool failures, incorrect outputs, and hallucinated actions. After UBFS extraction: 148 samples, binary labels.

\textbf{TRACE}~\cite{pan2024trace}: 517 agent trajectories capturing reward hacking in iterative self-refinement. Agents produce correct outputs through illegitimate means---structurally normal traces with semantically wrong behaviour. After UBFS extraction: 517 samples, binary labels.

\textbf{Synthetic OWASP}: Anomalies injected into TRAIL traces to simulate five OWASP ASI categories (ASI01 Agent Goal Hijack, ASI02 Tool Misuse, ASI05 Memory Poisoning, ASI09 Excessive Agency, ASI10 Rogue Agents). With anomaly ratio 0.5: ${\sim}$30 anomalous samples per category per seed. The injection method modifies UBFS features to match the expected structural signature of each attack type---e.g., ASI05 corrupts volume and output features; ASI02 modifies only privilege features while preserving all behavioural structure.

\textbf{ATBench}~\cite{atbench2026}: 500 multi-turn agent trajectories with tool calls, binary safety labels, and fine-grained risk categorisation mapped to OWASP ASI categories (ASI01, ASI02, ASI04, ASI05, ASI06, ASI10). After UBFS extraction: 500 samples. Used for MCP transfer validation (Experiment~7) and hybrid detection (Experiment~8).

\textbf{AgentHarm}~\cite{andriushchenko2024agentharm}: 440 malicious agent tasks in two configurations: direct (single-turn malicious request) and decomposed (same objective split into innocent-appearing subtasks). Used for decomposition evasion analysis (Experiment~6).

\textbf{Distillation sub-profiles}: Four synthetic attack profiles modelling the Anthropic-disclosed distillation strategies~\cite{anthropic2026distillation}: FOCUSED (MiniMax-style narrow extraction, extreme FREQUENCY spike), BROAD (Moonshot-style multi-capability probing, extreme SCOPE anomaly), COT (DeepSeek-style chain-of-thought elicitation, low-entropy SEQUENCE signature), and HYDRA (distributed across ${\sim}$20,000 accounts, per-account behaviour within normal bounds). Each is injected via the same synthetic generation framework as the OWASP profiles.

The scale asymmetry is dramatic: CMU-CERT has 329,000 user-days; TRAIL has 148 traces; TRACE has 517. This is by design---we are testing whether \textit{large-scale insider threat data} can bootstrap detection in \textit{small-scale agent monitoring}, a realistic deployment scenario where agent trace datasets are nascent.

\subsection{Models}
\label{sec:models}

We evaluate three models, validated at up to 0.807 AUC-ROC on CMU-CERT in prior work~\cite{rimal2026insider}:

\textbf{Isolation Forest}~\cite{liu2008isolation}: 200 trees, auto contamination, threshold at 95th percentile. Scores based on average path length---anomalies are geometrically easier to isolate.

\textbf{LSTM Autoencoder}~\cite{malhotra2016lstm, hochreiter1997lstm}: Encoder [64, 32] $\to$ 16-dim latent $\to$ decoder [32, 64]. MSE reconstruction loss, 100 epochs, early stopping (patience 15). Learns temporal dependencies in behavioural sequences.

\textbf{Deep Clustering}~\cite{xie2024deep}: Autoencoder [128, 64] $\to$ 32-dim latent $\to$ 5 KMeans clusters. Dual anomaly score combining reconstruction error and cluster distance. Captures latent structure that neither pure reconstruction nor pure clustering alone can model. For small datasets (e.g., TRAIL with only 4 normal training samples after splitting), the number of clusters is capped at the training set size.

Each model represents a different assumption about what makes behaviour anomalous. Isolation Forest asks ``is this point geometrically isolated?'' The LSTM Autoencoder asks ``is this sequence temporally unusual?'' Deep Clustering asks ``does this point belong to a coherent group?'' Testing all three reveals whether transfer depends on the detection mechanism or the feature schema.

\subsection{Experimental Protocol}
\label{sec:protocol}

We run thirteen experiments, each with 5 random seeds (42--46). Experiments 1--4 address \RQ{1}--\RQ{3} (cross-domain transfer, detection mapping, governance assumptions). Experiments 5--8 address \RQ{4} (real-world attack validation and semantic augmentation). Experiments 9--11 stress-test robustness, temporal sensitivity, and taxonomic generality. Experiments 12--13 provide real-data validation and sensitivity analysis.

\textbf{Experiment~1} (Within-domain baselines): Train and evaluate each model within each domain separately. Establishes what is detectable in isolation. [\RQ{1}]

\textbf{Experiment~2} (Cross-domain transfer): Train on one domain, evaluate on another. Tests whether anomaly knowledge transfers across the insider/agent boundary via UBFS. [\RQ{1}]

\textbf{Experiment~3} (OWASP detection matrix): Evaluate all three models on synthetic OWASP anomalies (${\sim}$30 per category, 5 seeds) injected into TRAIL traces, with bootstrap 95\% confidence intervals and Wilcoxon signed-rank tests between detection tiers. Maps detections to the OWASP Top~10 for Agentic Applications. [\RQ{2}]

\textbf{Experiment~4} (Governance audit): Compare feature distributions across domains using KL divergence. Identifies which normality assumptions transfer and what biases they carry. [\RQ{3}]

\textbf{Experiment~5} (Distillation detection): Evaluate detection of four distillation sub-profiles (FOCUSED, BROAD, COT, HYDRA) alongside the ASI02 baseline, testing whether per-account behavioural monitoring can detect distributed extraction. [\RQ{4}]

\textbf{Experiment~6} (Decomposition evasion): Compare detection of direct malicious requests against decomposed multi-step versions of the same attacks using AgentHarm, quantifying the ``boiling frog'' evasion effect at the prompt level. [\RQ{4}]

\textbf{Experiment~7} (MCP transfer): Extend cross-domain transfer to ATBench MCP tool-calling traces (IF and DC\footnote{Experiment~7 reports IF and DC only. The LSTM Autoencoder operates on pseudo-sequences of length~1 for these cross-domain transfer features, providing no temporal modelling advantage.}), testing whether the UBFS bridge generalises beyond the original TRAIL/TRACE/CERT datasets. [\RQ{1}, \RQ{2}]

\textbf{Experiment~8} (Hybrid detection): Augment UBFS-20 with 8 semantic features (UBFS-28) using sentence-transformer embeddings~\cite{reimers2019sentbert}, testing whether intent-aware features can close the ASI02 blind spot. Also measures whether semantic features help or degrade cross-domain transfer. [\RQ{4}]

\textbf{Experiment~9} (Adversarial evasion): Apply three evasion strategies---feature clamping, gradual escalation, and nearest-neighbour mimicry---to OWASP anomalies, quantifying detection robustness and the inverse relationship between baseline detectability and evasion vulnerability. [\RQ{2}, \RQ{4}]

\textbf{Experiment~10} (Temporal window ablation): Vary the observation window (5, 10, 20, full trace spans) on TRAIL and ATBench, testing whether the insider-domain finding that short windows suffice transfers to agent monitoring. [\RQ{1}]

\textbf{Experiment~11} (MITRE ATLAS mapping): Map five ATLAS adversarial ML techniques~\cite{mitre2024atlas} to UBFS perturbation profiles, testing whether the detection framework generalises beyond OWASP to the broader adversarial ML taxonomy. [\RQ{2}]

\textbf{Experiment~12} (Real-data OWASP validation): Evaluate all three models on 500 real ATBench trajectories per OWASP category, comparing UBFS-20 and UBFS-28. Computes Spearman rank correlation between synthetic (Experiment~3) and real-data detection rankings to quantify synthetic methodology bias. [\RQ{2}, \RQ{4}]

\textbf{Experiment~13} (Distillation sensitivity): Scale distillation attack intensity in feature space at [0.25$\times$, 0.5$\times$, 1.0$\times$, 2.0$\times$] for all four sub-profiles, measuring how detection degrades with lower attack intensity. [\RQ{4}]

For all experiments, models are trained on the ``normal'' subset of the source domain and evaluated on the full target domain (normal + anomalous). AUC-ROC is the primary metric, enabling comparison across datasets with different class balances. We also report Recall@5\%FPR and Recall@10\%FPR for experiments where these thresholds are operationally meaningful.

% ============================================================================
\section{Results}
\label{sec:results}

\subsection{Within-Domain Baselines}
\label{sec:baselines}

Table~\ref{tab:baselines} shows within-domain performance. Three patterns emerge.

\begin{table}[htbp]
\caption{Within-Domain Baselines (5 seeds, AUC-ROC $\pm$ std)}
\label{tab:baselines}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{TRAIL} & \textbf{TRACE} & \textbf{CMU-CERT} \\
\midrule
Isolation Forest & $0.577 \pm .021$ & $0.501 \pm .002$ & $\mathbf{0.731 \pm .009}$ \\
LSTM Autoencoder & $0.685 \pm .011$ & $0.521 \pm .003$ & $0.723 \pm .038$ \\
Deep Clustering & $\mathbf{0.897 \pm .021}$ & $0.496 \pm .002$ & $0.697 \pm .014$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{TRAIL is detectable.} Deep Clustering achieves 0.897 AUC-ROC on agent execution traces---the latent cluster structure separates normal traces from error traces effectively. The LSTM Autoencoder captures temporal patterns (0.685), while Isolation Forest provides moderate detection (0.577).

\textbf{TRACE is not.} All models hover at random chance (${\sim}$0.50). Reward hacking produces traces that are structurally identical to normal traces---the agent calls the same tools in the same order, it just achieves the right answer through the wrong process. This is a \textit{semantic} anomaly, not a \textit{behavioural} one. No amount of structural monitoring will catch it.

\textbf{CMU-CERT works through UBFS.} Isolation Forest achieves 0.731, lower than 0.807 with the full 24-feature pipeline~\cite{rimal2026insider}. The 9.4\% drop is the cost of the lossy UBFS compression---trading detection fidelity for cross-domain portability.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig2_roc_curves.png}
\caption{ROC curves for within-domain baselines across three domains. TRAIL shows strong separation (Deep Clustering dominates); TRACE clusters at the diagonal (structurally undetectable); CMU-CERT shows moderate detection through the UBFS lens.}
\label{fig:roc_curves}
\end{figure}

\subsection{Cross-Domain Transfer}
\label{sec:transfer}

This is the central experiment. Table~\ref{tab:transfer} shows cross-domain AUC-ROC and the drop from within-domain performance.

\begin{table}[htbp]
\caption{Cross-Domain Transfer (AUC-ROC)}
\label{tab:transfer}
\centering
\small
\begin{tabular}{llccc}
\toprule
\textbf{Direction} & \textbf{Model} & \textbf{Source} & \textbf{Target} & \textbf{Drop} \\
\midrule
CERT $\to$ TRAIL & IF & 0.731 & \textbf{0.711} & \textbf{--0.019} \\
CERT $\to$ TRAIL & DC & 0.688 & 0.632 & --0.056 \\
\addlinespace
CERT $\to$ TRACE & IF & 0.731 & 0.501 & --0.230 \\
CERT $\to$ TRACE & DC & 0.688 & 0.499 & --0.188 \\
\addlinespace
TRACE $\to$ CERT & IF & 0.501 & 0.535 & +0.035 \\
TRACE $\to$ CERT & DC & 0.496 & \textbf{0.719} & \textbf{+0.223} \\
\addlinespace
TRAIL $\to$ CERT & IF & 0.577 & 0.525 & --0.053 \\
TRAIL $\to$ CERT & DC & 0.897 & 0.640 & --0.257 \\
\addlinespace
TRAIL $\to$ TRACE & IF & 0.577 & 0.469 & --0.109 \\
TRAIL $\to$ TRACE & DC & 0.897 & 0.513 & --0.384 \\
\addlinespace
TRACE $\to$ TRAIL & IF & 0.501 & 0.614 & +0.113 \\
TRACE $\to$ TRAIL & DC & 0.496 & 0.631 & +0.135 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{The headline: CERT $\to$ TRAIL retains 97\% of detection power.} An Isolation Forest trained entirely on 329,000 insider threat user-days achieves 0.711 AUC-ROC on agent execution traces---a drop of only 0.019 from its within-domain performance of 0.731 (std $\pm$0.009 within-domain, $n{=}5$ seeds). Effect sizes are large where transfer succeeds: TRACE $\to$ CERT DC gains +0.223, exceeding within-domain variance by an order of magnitude. The UBFS bridge works: anomaly knowledge learned from employee behaviour transfers directly to agent monitoring.

\textbf{The unexpected result: TRACE $\to$ CERT improves Deep Clustering by +0.223.} Training on 517 reward-hacking trajectories (a domain where DC achieves only 0.496) produces a richer normal baseline that \textit{outperforms within-domain DC} on insider detection (0.719 vs.\ 0.697). More training data from \textit{any} domain helps, even across the insider/agent boundary.

\textbf{Agent-to-agent transfer reveals asymmetry.} TRACE $\to$ TRAIL improves both models (IF: +0.113, DC: +0.135) despite TRACE having no within-domain detection power. The mechanism is data augmentation: TRACE's 517 trajectories provide a richer characterisation of ``normal'' agent behaviour than TRAIL's 148 traces alone. Training on a larger normal baseline, even from a different agent task, sharpens the boundary between normal and anomalous.

\textbf{Nothing transfers to TRACE.} CERT $\to$ TRACE drops to 0.501 (IF) and 0.499 (DC). TRAIL $\to$ TRACE drops to 0.469 and 0.513. This is consistent with the within-domain result: reward hacking is structurally undetectable regardless of training domain. This is not a failure of transfer---it is a category error. Reward hacking is a semantic anomaly (the agent achieves the right answer through the wrong process), and no amount of behavioural feature engineering will surface it. Detecting reward hacking requires output verification, not input monitoring.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_feature_heatmap.png}
\caption{Feature activation heatmap across domains. Despite different source data, UBFS produces comparable feature structure---the basis for cross-domain transfer.}
\label{fig:heatmap}
\end{figure}

\subsection{OWASP Detection Matrix}
\label{sec:owasp}

Table~\ref{tab:owasp} maps detection performance to OWASP ASI categories, revealing three distinct tiers. All three models are evaluated across 5 seeds with bootstrap 95\% confidence intervals; Wilcoxon signed-rank tests confirm each tier boundary is statistically significant ($p{=}0.031$ for all adjacent pairs).

\begin{table}[htbp]
\caption{OWASP ASI Detection Matrix (Synthetic, AUC-ROC $\pm$ std, 5 seeds)}
\label{tab:owasp}
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{OWASP Category} & \textbf{IF} & \textbf{LSTM} & \textbf{DC} \\
\midrule
ASI05 Mem.\ Poisoning & $.920{\pm}.010$ & $\mathbf{.975{\pm}.006}$ & $.981{\pm}.013$ \\
ASI09 Excess.\ Agency & $.858{\pm}.019$ & $.831{\pm}.014$ & $.852{\pm}.020$ \\
ASI10 Rogue Agents & $.729{\pm}.021$ & $.696{\pm}.035$ & $.730{\pm}.032$ \\
ASI01 Goal Hijack & $.642{\pm}.034$ & $.637{\pm}.031$ & $.621{\pm}.039$ \\
\addlinespace
ASI02 Tool Misuse & $.520{\pm}.025$ & $.510{\pm}.019$ & $.524{\pm}.037$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Tier 1: Detectable (ASI05).} Memory poisoning produces dramatic structural anomalies---corrupted outputs, unusual token volumes, broken execution patterns. All models detect it at 0.92 or above, with LSTM achieving 0.975 [CI: 0.970, 0.980].

\textbf{Tier 2: Moderate (ASI09, ASI10, ASI01).} Excessive agency and rogue agent behaviour produce measurable structural deviations: more tool calls, broader scope, higher privilege. Detection ranges from 0.62 to 0.86, sufficient for triage but not reliable standalone.

\textbf{Tier 3: Blind spot (ASI02).} Tool Misuse achieves only 0.51--0.52 AUC-ROC---barely above chance. The agent calls the right tools in the right order with the right sequencing; it simply uses escalated privileges or targets unauthorised resources. The behavioural \textit{structure} is identical to legitimate use. This is the AI equivalent of an insider using their real credentials to do something illegitimate---structural anomaly detection cannot distinguish it from normal work. However, as Experiment~12 reveals, this blind spot is partially an artifact of circular synthetic methodology (Section~\ref{sec:real_owasp}).

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_owasp_matrix.png}
\caption{OWASP detection heatmap. ASI05 (Memory Poisoning) is reliably detected; ASI02 (Tool Misuse) is a structural blind spot. Categories ASI03--04 and ASI06--08 were not testable with current synthetic generation.}
\label{fig:owasp}
\end{figure}

Five additional OWASP categories (ASI03 Identity \& Privilege Abuse, ASI04 Cascading Hallucinations, ASI06 Supply Chain Compromise, ASI07 Unsafe Code Generation, ASI08 Inadequate Sandboxing) could not be tested with the current synthetic generation framework. These categories require richer simulation of multi-agent interactions, network-level features, or supply chain dependencies that lie outside the UBFS's behavioural scope.

The parallel to insider threat detection is exact. In~\cite{rimal2026insider}, Scenario~1 attacks (USB exfiltration with after-hours access) were detected at 69\%, while Scenario~2 attacks (quiet email/HTTP exfiltration) were detected at only 3\%. The OWASP results mirror this: structurally loud attacks (ASI05, ASI09) are detectable; structurally quiet attacks (ASI02) are not. The ``boiling frog'' pattern---threats that operate within legitimate behavioural boundaries---is a fundamental limitation of anomaly-based detection in both domains.

This has direct governance implications: any monitoring system built purely on behavioural patterns will miss the most sophisticated attacks---precisely the ones that matter most.

\subsection{Latent Space Structure}
\label{sec:latent}

Visualising the Deep Clustering latent space reveals how the models organise cross-domain data (Figs.~\ref{fig:scores}--\ref{fig:tsne}).

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_score_distributions.png}
\caption{Anomaly score distributions for normal vs.\ anomalous samples across domains. TRAIL shows clear separation; TRACE distributions overlap almost completely.}
\label{fig:scores}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig5_tsne_latent.png}
\caption{t-SNE of Deep Clustering latent space. Left: coloured by domain---TRAIL and TRACE cluster separately despite sharing the UBFS representation. Right: coloured by anomaly status---anomalies scatter across both domains rather than forming a universal cluster. Anomalies are defined by deviation from \textit{domain-specific} baselines.}
\label{fig:tsne}
\end{figure}

The t-SNE visualisation (Fig.~\ref{fig:tsne}) shows that domains cluster separately in latent space, confirming that UBFS preserves domain-specific structure. Anomalous samples do not form their own cluster---they scatter, defined by deviation from domain-specific baselines rather than by a universal ``anomalous'' signature. This explains why transfer works for IF (which learns global isolation) but varies for DC (which relies on cluster structure that differs across domains).

\subsection{Distillation Attack Spectrum}
\label{sec:distillation}

Experiment~5 tests whether behavioural monitoring detects model distillation attacks---the extraction strategy disclosed by Anthropic in February 2026~\cite{anthropic2026distillation}. We evaluate four sub-profiles modelling distinct extraction strategies, alongside ASI02 as a known blind-spot baseline. Table~\ref{tab:distillation} reveals a detection spectrum spanning near-perfect to near-random.

\begin{table}[htbp]
\caption{Distillation Detection Spectrum (AUC-ROC, 5 seeds)}
\label{tab:distillation}
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Profile} & \textbf{IF} & \textbf{LSTM} & \textbf{DC} \\
\midrule
FOCUSED (MiniMax) & $\mathbf{0.995}$ & $\mathbf{0.999}$ & $\mathbf{0.936}$ \\
BROAD (Moonshot) & $0.902$ & $0.913$ & $0.908$ \\
COT (DeepSeek) & $0.909$ & $0.805$ & $0.544$ \\
HYDRA (distributed) & $0.538$ & $0.538$ & $0.540$ \\
\addlinespace
ASI02 Tool Misuse & $0.497$ & $0.500$ & $0.491$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{FOCUSED extraction is trivially detectable.} The MiniMax pattern---13M+ exchanges on narrow coding targets---produces extreme FREQUENCY anomalies. All three models detect it above 0.93. This is the ``smash and grab'' of model extraction: fast, effective, and loud.

\textbf{BROAD and COT produce detectable but distinct signatures.} Moonshot's multi-capability probing creates SCOPE anomalies (extreme resource breadth); all three models detect it consistently (${\sim}$0.90). DeepSeek's chain-of-thought elicitation creates SEQUENCE anomalies (low entropy, high repetition). IF and LSTM detect this well (0.91, 0.81), but Deep Clustering drops to 0.544---cluster structure does not capture the repetitive prompt pattern.

\textbf{HYDRA is a second structural blind spot.} Per-account behaviour in a distributed cluster (20,000+ accounts) falls within normal bounds. All three models achieve only ${\sim}$0.54---marginally better than ASI02. But the failure mode is different: ASI02 is an \textit{intent} blind spot (right tools, wrong purpose); HYDRA is a \textit{coordination} blind spot (normal per-entity, anomalous in aggregate). Behavioural monitoring has two distinct structural ceilings, not one.

\subsection{Decomposition Evasion}
\label{sec:decomposition}

Experiment~6 tests whether splitting a malicious objective into innocent-appearing subtasks---the strategy used in the GTG-1002 espionage campaign---reduces detection. Using AgentHarm's direct and decomposed configurations, we measure the detection drop across all three models. Table~\ref{tab:decomposition} shows the results.

\begin{table}[htbp]
\caption{Decomposition Evasion (AUC-ROC, 5 seeds)}
\label{tab:decomposition}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Direct} & \textbf{Decomposed} & \textbf{$\Delta$} & \textbf{$p$} \\
\midrule
Isolation Forest & $0.774$ & $0.723$ & $-0.051$ & $0.031^{*}$ \\
LSTM Autoencoder & $0.718$ & $0.711$ & $-0.007$ & $0.094$ \\
Deep Clustering & $0.774$ & $0.710$ & $-0.064$ & $0.031^{*}$ \\
\bottomrule
\multicolumn{5}{l}{\footnotesize $^{*}p{<}0.05$, Wilcoxon signed-rank, one-sided} \\
\end{tabular}
\end{table}

Decomposition costs 5--6\% detection power for IF and DC, with both drops statistically significant ($p{=}0.031$, Wilcoxon signed-rank). LSTM shows near-invariance ($\Delta{=}-0.007$, $p{=}0.094$, not significant). The pattern mirrors insider threat evasion: in~\cite{rimal2026insider}, gradual escalation (Scenario~2) dropped detection from 69\% to 3\%. The agent-domain effect is smaller in absolute terms because the direct attacks are already harder to detect (0.72--0.77 vs.\ Scenario~1's 0.69 recall). But the mechanism is identical---spreading malicious activity across multiple steps reduces each step's anomaly score below the detection threshold.

The LSTM's robustness to decomposition is notable and statistically distinguishable from IF/DC. Temporal models capture the \textit{cumulative} effect of a sequence rather than scoring individual actions. A decomposed attack still produces an unusual trajectory even if each step appears innocent. This suggests that sequence-aware models may be more resistant to ``boiling frog'' evasion than point-in-time detectors.

\subsection{MCP Transfer Generalisation}
\label{sec:mcp}

Experiment~7 extends cross-domain transfer to ATBench, a benchmark of 500 MCP tool-calling trajectories with fine-grained risk labels. Table~\ref{tab:mcp} shows within-domain and transfer results.

\begin{table}[htbp]
\caption{MCP Transfer (ATBench, AUC-ROC, 5 seeds)}
\label{tab:mcp}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Setting} & \textbf{IF} & \textbf{DC} \\
\midrule
ATBench within-domain & $0.838$ & $\mathbf{0.890}$ \\
TRAIL $\to$ ATBench & $\mathbf{0.878}$ & $0.737$ \\
ATBench $\to$ TRAIL & $0.774$ & $0.734$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{TRAIL $\to$ ATBench IF exceeds within-domain performance.} At 0.878 AUC-ROC, the transferred Isolation Forest outperforms the within-domain IF (0.838) by 4.8\%---a 104.8\% retention rate. This extends the Experiment~2 finding: not only does anomaly knowledge transfer across the insider/agent boundary, it transfers across \textit{agent benchmark boundaries}. The mechanism is the same as TRACE $\to$ CERT in Experiment~2---the source domain's training data provides a richer characterisation of ``normal'' than the target domain alone.

Per-risk-category detection on ATBench (Table~\ref{tab:mcp_risk}) reveals a more nuanced OWASP picture than the synthetic injection results of Experiment~3.

\begin{table}[htbp]
\caption{Per-Risk Detection on ATBench (IF, AUC-ROC)}
\label{tab:mcp_risk}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{OWASP Category} & \textbf{AUC-ROC} & \textbf{$n$} \\
\midrule
ASI05 Memory Poisoning & $0.878$ & 30 \\
ASI06 Supply Chain & $0.864$ & 43 \\
ASI02 Tool Misuse & $0.860$ & 58 \\
ASI01 Goal Hijack & $0.844$ & 61 \\
ASI04 Cascading Hallucination & $0.776$ & 29 \\
ASI10 Rogue Agents & $0.765$ & 29 \\
\bottomrule
\end{tabular}
\end{table}

Notably, ASI02 achieves 0.860 on ATBench---far above the 0.57--0.59 seen with synthetic injection in Experiment~3. This suggests that ATBench's Tool Misuse examples produce more structurally visible anomalies than our synthetic profiles, which were designed to be maximally stealthy. Real-world tool misuse may occupy a middle ground between the synthetic worst-case and easily detectable cases.

\subsection{Hybrid Detection with Semantic Features}
\label{sec:hybrid}

Experiment~8 tests the obvious remedy for structural blind spots: adding semantic features. We augment UBFS-20 with 8 intent-aware dimensions (parameter embedding distance, intent consistency, escalation gradient, parameter anomaly, context-tool alignment, goal drift, semantic novelty, instruction compliance) computed via sentence-transformer embeddings~\cite{reimers2019sentbert}. Table~\ref{tab:hybrid} compares UBFS-20 and UBFS-28 across all OWASP categories using the best-performing model per category.

\begin{table}[htbp]
\caption{UBFS-20 vs.\ UBFS-28 Detection (AUC-ROC, best model per row)}
\label{tab:hybrid}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Category} & \textbf{-20} & \textbf{-28} & \textbf{$\Delta$} & \textbf{Best} \\
\midrule
ASI01 Goal Hijack & .627 & $\mathbf{.935}$ & +.308 & LSTM \\
ASI10 Rogue Agents & .670 & $\mathbf{.939}$ & +.269 & LSTM \\
ASI09 Excess.\ Agency & .814 & $\mathbf{.951}$ & +.138 & LSTM \\
ASI05 Mem.\ Poisoning & $\mathbf{.989}$ & .976 & --.013 & DC \\
\addlinespace
ASI02 Tool Misuse & .537 & .559 & +.023 & DC \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Semantic features transform detection for four categories.} ASI01 jumps from 0.63 to 0.94 (LSTM, $+0.31$); ASI10 from 0.67 to 0.94 ($+0.27$); ASI09 from 0.81 to 0.95 ($+0.14$). Intent consistency and goal drift scores expose the semantic mismatch that behavioural features miss. ASI05, already near-perfect, shows no improvement. All values in Table~\ref{tab:hybrid} use a single model per row (the best UBFS-28 performer) to ensure the delta is a fair within-model comparison.

\textbf{ASI02 remains a blind spot.} Despite adding 8 semantic dimensions specifically designed to capture intent signals, Tool Misuse moves only from ${\sim}$0.54 to ${\sim}$0.56 (best model DC: 0.537 $\to$ 0.559). The agent's stated intent, tool parameters, and action sequence are all \textit{consistent} with legitimate use. The misuse is in the authorisation context---who gave the instruction and whether it was authorised---which neither behavioural nor semantic features capture. This confirms ASI02 as a structural limitation of monitoring-based detection: closing it requires authorisation verification, not better features.

\textbf{Transfer degrades with UBFS-28.} Adding semantic features to cross-domain transfer (TRAIL $\to$ ATBench) reduces IF from 0.878 to 0.799 ($-0.079$) and DC from 0.737 to 0.609 ($-0.129$). The semantic dimensions are domain-specific---intent consistency scores learned from TRAIL traces do not align with ATBench traces. This creates a portability-accuracy trade-off: UBFS-20 transfers better, UBFS-28 detects better within-domain. Practitioners must choose.

\subsection{Adversarial Evasion Testing}
\label{sec:evasion}

Experiment~9 tests three evasion strategies against UBFS-based monitoring, formalising the ``boiling frog'' pattern from~\cite{rimal2026insider} into a measurable adversarial framework.

\textbf{Feature clamping} caps anomalous features at the 95th percentile of the normal distribution, simulating an attacker who knows the monitoring thresholds. \textbf{Gradual escalation} linearly interpolates from a normal profile to an anomalous target in $N$ steps. \textbf{Mimicry} replaces all features except the privilege dimension with the nearest normal neighbour's values, preserving the attack payload while making everything else look normal.

\begin{table}[htbp]
\caption{Adversarial Evasion: AUC-ROC Drop (\%) by Strategy}
\label{tab:evasion}
\centering
\small
\begin{tabular}{@{}llrrrrr@{}}
\toprule
\textbf{Strategy} & \textbf{Model} & \textbf{ASI01} & \textbf{ASI02} & \textbf{ASI05} & \textbf{ASI09} & \textbf{ASI10} \\
\midrule
\multirow{3}{*}{Clamp}
 & IF   &  5.1 &  1.5 &  7.9 &  2.5 &  2.8 \\
 & LSTM &  2.9 &  1.9 & \textbf{20.9} &  6.8 &  3.8 \\
 & DC   & --4.1 &  1.4 & 10.0 &  1.4 & --0.1 \\
\addlinespace
\multirow{3}{*}{Mimicry}
 & IF   &  9.6 &  0.0 &  6.1 & 14.6 & 12.4 \\
 & LSTM &  8.4 &  0.0 & \textbf{25.1} & 14.6 & 11.3 \\
 & DC   &  4.8 & --1.0 & 14.4 & 14.4 & 11.8 \\
\bottomrule
\end{tabular}
\end{table}

Three findings emerge from Table~\ref{tab:evasion}. \textbf{ASI05 is most vulnerable.} Memory Poisoning relies on volume spikes that are easily capped; clamping drops LSTM detection by 21\%, and mimicry drops it by 25\%. This category's high baseline AUC-ROC (0.976) is artificially inflated by the same feature visibility that makes it easy to evade.

\textbf{ASI02 is immune to evasion}---because there is nothing to evade. All three strategies produce $<$2\% change, confirming that ASI02 is already at the detection floor. You cannot degrade what is already at chance.

\textbf{Gradual escalation reveals a sharp threshold.} Only ASI05 triggers detection before full escalation ($\alpha = 0.64$ for IF, $\alpha = 0.92$ for LSTM). All other categories require $\alpha = 1.0$---the complete anomalous profile---before crossing the detection threshold. This means behavioural monitoring is binary, not gradual: either the anomaly is large enough to detect, or it is invisible. There is no ``early warning'' for subtle attacks.

\subsection{Temporal Window Ablation}
\label{sec:temporal}

Experiment~10 tests whether the optimal monitoring window transfers across domains. In~\cite{rimal2026insider}, 7-day windows outperform 14 and 30-day windows for insider detection. We test analogous span windows (5, 10, 20, full trace) on TRAIL and ATBench.

\begin{table}[htbp]
\caption{Temporal Window Ablation (AUC-ROC)}
\label{tab:temporal}
\centering
\small
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{5} & \textbf{10} & \textbf{20} & \textbf{Full} \\
\midrule
\multirow{3}{*}{TRAIL}
 & IF   & .543 & \textbf{.581} & .505 & .577 \\
 & LSTM & .479 & .537 & .509 & \textbf{.685} \\
 & DC   & .842 & .829 & .851 & \textbf{.897} \\
\addlinespace
\multirow{3}{*}{ATBench}
 & IF   & .783 & \textbf{.838} & .838 & .838 \\
 & LSTM & .720 & \textbf{.781} & .781 & .781 \\
 & DC   & .701 & \textbf{.891} & .891 & .891 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Ten spans is the saturation point for ATBench.} All three models plateau at 10 spans, with no improvement from 20 or full traces (Table~\ref{tab:temporal}). ATBench trajectories average 8.97 turns---10 spans captures the complete behavioural signature. Beyond saturation, additional spans add noise, not signal.

\textbf{TRAIL shows model-dependent optima.} Isolation Forest peaks at 10 spans (0.581 vs.\ 0.577 at full), but LSTM and DC perform best on full traces. TRAIL's error patterns are distributed throughout execution; temporal models benefit from the complete sequence, while IF's isolation geometry stabilises earlier.

\textbf{The structural insight transfers.} In~\cite{rimal2026insider}, 7 days (23\% of the 30-day maximum) is optimal for Isolation Forest. Here, 10 spans is optimal for IF on both datasets. The pattern is consistent: simpler models extract sufficient signal from early observations, while sequence-aware models benefit from longer histories. Monitoring systems should use adaptive windows---short for threshold-based detection, long for temporal anomaly detection.

\subsection{MITRE ATLAS Mapping}
\label{sec:atlas}

Experiment~11 tests whether the UBFS detection framework generalises beyond OWASP to the MITRE ATLAS adversarial ML taxonomy~\cite{mitre2024atlas}. We map five ATLAS techniques to UBFS perturbation profiles and evaluate detection using the same synthetic injection pipeline as Experiment~3.

\begin{table}[htbp]
\caption{MITRE ATLAS Detection Matrix (AUC-ROC, mean $\pm$ std)}
\label{tab:atlas}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{ATLAS Technique} & \textbf{IF} & \textbf{LSTM} & \textbf{DC} \\
\midrule
T0044 Replication & $.993{\pm}.003$ & $.998{\pm}.001$ & $.909{\pm}.024$ \\
T0048 Extraction  & $.990{\pm}.005$ & $.990{\pm}.006$ & $.880{\pm}.041$ \\
T0043 Adversarial & $.956{\pm}.009$ & $.931{\pm}.011$ & $.779{\pm}.074$ \\
T0025 Exfiltration & $.993{\pm}.004$ & $.996{\pm}.003$ & $.899{\pm}.030$ \\
T0042 Verify      & $.975{\pm}.007$ & $.964{\pm}.013$ & $.830{\pm}.051$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{All five ATLAS techniques are strongly detectable.} Isolation Forest and LSTM Autoencoder achieve $>$0.93 AUC-ROC across all techniques (Table~\ref{tab:atlas}). Deep Clustering is weaker but still above 0.78. This is because ATLAS techniques---model replication, extraction, adversarial data crafting---have strong FREQUENCY and VOLUME signatures. They require many queries, high data throughput, and systematic access patterns. These are exactly the dimensions where UBFS-based detection excels.

\textbf{No new blind spots.} The cross-taxonomy comparison reveals that all five ATLAS techniques land in the ``strong'' detection tier ($>$0.80 for IF). The OWASP blind spot (ASI02: Tool Misuse) has no ATLAS equivalent---ATLAS techniques are volume-intensive by nature, making them structurally visible. The only blind spot in UBFS-based detection remains intent-based: attacks that use the right tools in the right way for the wrong reasons.

\textbf{T0043 (Adversarial Data Crafting) is the weakest.} At 0.779 AUC-ROC for DC, it is the closest ATLAS technique to the moderate tier. Its SCOPE signature (broad resource exploration) partially overlaps with normal multi-tool agent behaviour, making it harder to distinguish. This is the ATLAS analogue of ASI01 (Goal Hijacking)---both involve diverse tool usage that could be legitimate.

\subsection{Real-Data OWASP Validation}
\label{sec:real_owasp}

Experiment~12 tests whether the synthetic OWASP detection tiers from Experiment~3 hold on real data. The synthetic methodology is circular: perturbation profiles define exact UBFS modifications, then models ``detect'' those same modifications. ATBench provides 500 real agent trajectories with ground-truth OWASP labels, breaking this circularity.

\begin{table}[htbp]
\caption{Real-Data OWASP Detection on ATBench (AUC-ROC, 5 seeds)}
\label{tab:real_owasp}
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{Feat.} & \textbf{ASI01} & \textbf{ASI02} & \textbf{ASI05} & \textbf{ASI06} & \textbf{ASI10} \\
\midrule
\multirow{2}{*}{IF}
 & -20 & .843 & .860 & .877 & .863 & .765 \\
 & -28 & $\mathbf{.943}$ & $\mathbf{.940}$ & $\mathbf{.934}$ & $\mathbf{.942}$ & $\mathbf{.877}$ \\
\addlinespace
\multirow{2}{*}{LSTM}
 & -20 & .785 & .812 & .800 & .796 & .714 \\
 & -28 & .857 & .860 & .811 & .835 & .823 \\
\addlinespace
\multirow{2}{*}{DC}
 & -20 & .894 & .903 & .915 & .900 & .851 \\
 & -28 & .926 & .921 & .903 & .914 & .873 \\
\bottomrule
\end{tabular}
\end{table}

Three findings emerge from Table~\ref{tab:real_owasp}.

\textbf{The ASI02 blind spot is a synthetic artifact.} On real ATBench data, ASI02 achieves 0.81--0.94 AUC-ROC across all models and feature sets---far above the 0.51--0.52 seen with synthetic injection in Experiment~3. Real-world tool misuse produces structurally visible anomalies (unusual parameter combinations, atypical tool sequences) that the circular synthetic profiles---designed to modify only the privilege dimension---cannot capture.

\textbf{UBFS-28 improves real-data detection.} Semantic features add up to 13\% for Isolation Forest, with the largest gains on ASI10 (IF: $+$11.3\%) and ASI01 (IF: $+$10.0\%). Deep Clustering and LSTM show smaller but consistent gains (1--9\% across categories). Unlike the synthetic results of Experiment~8, where UBFS-28 failed to close ASI02, on real data UBFS-28 improves ASI02 by 1.8--8.1\% depending on model.

\textbf{Synthetic profiling has weak rank correlation with real detection.} Spearman $\rho$ between Experiment~3 and Experiment~12 per-category AUC-ROC is 0.20 (IF, $p{=}0.80$) and $-0.40$ (LSTM, $p{=}0.60$). The synthetic profiles systematically underestimate ASI01 and ASI02 while slightly overestimating ASI05. This validates the need for real-data benchmarks: synthetic OWASP profiling provides a useful lower bound but poor absolute calibration.

\subsection{Distillation Sensitivity}
\label{sec:sensitivity}

Experiment~13 tests how detection degrades as distillation attack intensity decreases, scaling perturbations in UBFS feature space: $\mathbf{x}_{\text{scaled}} = \boldsymbol{\mu}_{\text{normal}} + s \cdot (\mathbf{x}_{\text{attack}} - \boldsymbol{\mu}_{\text{normal}})$ for scale factors $s \in \{0.25, 0.5, 1.0, 2.0\}$.

\begin{table}[htbp]
\caption{Distillation Sensitivity (IF AUC-ROC, 5 seeds)}
\label{tab:sensitivity}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Profile} & $\mathbf{0.25\times}$ & $\mathbf{0.5\times}$ & $\mathbf{1.0\times}$ & $\mathbf{2.0\times}$ \\
\midrule
FOCUSED & $.959$ & $.979$ & $.995$ & $1.000$ \\
BROAD & $.789$ & $.835$ & $.919$ & $.965$ \\
COT & $.801$ & $.850$ & $.895$ & $.959$ \\
HYDRA & $.678$ & $.632$ & $.519$ & $.851$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{FOCUSED/BROAD/COT degrade monotonically.} As attack intensity decreases from $2.0\times$ to $0.25\times$, detection drops predictably (Table~\ref{tab:sensitivity}). FOCUSED remains highly detectable even at quarter intensity (IF: 0.959), confirming that massive-volume extraction cannot be made stealthy through attenuation. COT and BROAD converge toward 0.80 at $0.25\times$---still detectable, but approaching the moderate tier.

\textbf{HYDRA shows non-monotonic behaviour.} At $1.0\times$ (original profile), HYDRA achieves only 0.52---near chance, as expected for near-normal per-account behaviour. But at $0.25\times$ (even closer to normal), detection paradoxically improves to 0.68. The explanation: scaling toward the normal mean makes all anomalous samples cluster tightly around the centroid, creating an unusual density that Isolation Forest's path-length metric detects. At $2.0\times$, HYDRA overshoots normal bounds entirely (0.85), becoming trivially detectable. The practical implication: HYDRA's $1.0\times$ profile sits at the optimal evasion point.

% ============================================================================
\section{Governance Analysis}
\label{sec:governance}

\subsection{What ``Normal'' Encodes}
\label{sec:assumptions}

Every anomaly detection system makes assumptions about what ``normal'' means. These assumptions are not neutral---they encode values about acceptable behaviour, and they transfer across domains along with the detection models.

We identify six normality assumptions embedded in the UBFS (Table~\ref{tab:assumptions}).

\begin{table}[htbp]
\caption{Normality Assumptions in the UBFS}
\label{tab:assumptions}
\centering
\small
\begin{tabular}{p{1.8cm}p{2.6cm}p{2.6cm}}
\toprule
\textbf{Assumption} & \textbf{Insider Implication} & \textbf{Agent Implication} \\
\midrule
After-hours activity is suspicious & Penalises shift workers, caregivers, remote time zones & Penalises scheduled agents, asynchronous tasks \\
\addlinespace
Peer deviation is suspicious & Assumes group homogeneity; flags legitimate variation & Assumes agent-type baselines are stable \\
\addlinespace
Broad scope is suspicious & Privileges specialists over generalists & Flags multi-tool agents \\
\addlinespace
High volume means risk & Equates data movement with exfiltration & Equates token usage with misuse \\
\addlinespace
Unpredictable actions are suspicious & Penalises creative, exploratory work & Penalises novel problem-solving \\
\addlinespace
Above-role access is suspicious & Assumes stable role definitions & Assumes stable capability grants \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/fig6_governance_table.png}
\caption{Cross-domain comparison of normality assumptions. Each assumption encodes the same structural bias in both domains---the insider threat model's definition of ``suspicious'' maps directly onto the agent monitoring model's definition.}
\label{fig:governance}
\end{figure}

\subsection{Distribution Divergence}

The governance audit compares UBFS feature distributions between the CMU-CERT and agent domains using KL divergence. Of 20 features: 7 show zero variance in both domains (NaN divergence---features like \texttt{after\_hours\_ratio} and \texttt{weekend\_activity\_flag} are effectively unused); 6 show low divergence (KL $< 2.0$), indicating alignable distributions across domains; and 7 show high divergence (KL $> 7.8$), where cross-domain assumptions are most likely to introduce bias.

The highest-divergence features---\texttt{activity\_hour\_mean} (KL = 24.6), \texttt{output\_count\_norm} (KL = 22.1), \texttt{secondary\_event\_count} (KL = 21.0), \texttt{primary\_event\_count} (KL = 20.9), \texttt{event\_rate\_zscore} (KL = 19.8)---all have zero variance in the agent domain but rich variance in CERT. These are features where ``normal'' means fundamentally different things in each domain. An employee's typical event count reflects human work patterns; an agent's collapses to a constant. A model trained to flag unusual event counts in one domain may systematically misjudge the other.

The effective UBFS dimensionality is 13 (after excluding the 7 zero-variance features). Of these 13 active features, 6 align well across domains (KL $< 2.0$)---explaining why transfer works---while 7 diverge significantly---explaining why it does not work perfectly.

\subsection{The Portability Paradox}

The UBFS enables cross-domain transfer of detection models. It also enables cross-domain transfer of \textit{biases}. When an insider threat model penalises after-hours activity and that model is ported to agent monitoring, it will flag scheduled agents running overnight batch jobs. When it penalises broad resource access, it will flag multi-tool agents that legitimately need diverse capabilities.

This is not a bug in the UBFS---it is a structural consequence of any feature schema that enables transfer. The same representational bridge that carries detection knowledge also carries the assumptions baked into the training data. Governance frameworks must account for this: validating a model in one domain does not validate the assumptions it will impose in another.

The portability paradox extends beyond individual assumptions. The \textit{definition of peer group}---the reference population against which deviation is measured---differs fundamentally between domains. In insider threat detection, peer groups are defined by organisational role (all analysts, all engineers). In agent monitoring, they are defined by agent type (all coding assistants, all research agents). Both definitions are arbitrary, and both create structural biases against entities that do not fit neatly into predefined categories.

\subsection{Implications for AI Governance}

Three implications follow:

\textbf{1. Borrowed models carry borrowed assumptions.} Organisations adopting insider threat detection for agent monitoring inherit the security community's definition of ``suspicious.'' That definition was designed for employees, not agents. It should be audited before deployment.

\textbf{2. The blind spots are the same.} Tool Misuse (ASI02) evades detection for the same reason Scenario~2 insiders evade detection in~\cite{rimal2026insider}: the behaviour is structurally legitimate. Any governance framework that relies solely on behavioural anomaly detection will miss the most sophisticated threats in both domains.

\textbf{3. ``Normal'' is a policy choice.} Deciding what counts as normal behaviour---for employees or agents---is not a technical decision. It encodes organisational values about acceptable working patterns, appropriate resource access, and the boundary between legitimate flexibility and suspicious deviation. These choices should be made explicitly, not inherited implicitly from training data.

% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Why Transfer Works (and Doesn't)}

Transfer succeeds when the source and target domains share structural dimensions: temporal patterns, frequency distributions, scope metrics. The UBFS captures these shared dimensions, enabling Isolation Forest---which learns global isolation in feature space---to transfer nearly perfectly (CERT $\to$ TRAIL: 97\% retention).

The mechanism is instructive. Isolation Forest does not learn domain-specific patterns---it learns \textit{isolation geometry} in feature space. A point that is geometrically isolated in the UBFS space is isolated regardless of whether it represents an employee or an agent. This is why IF transfers better than Deep Clustering (which relies on domain-specific cluster structure) and LSTM Autoencoder (which relies on domain-specific temporal patterns). The simpler the model's assumptions, the more portable its knowledge.

Transfer fails when the anomaly is semantic rather than structural. Reward hacking (TRACE) produces traces that are structurally normal but semantically wrong. No amount of training data from any domain will detect this through behavioural features alone. Similarly, Tool Misuse (ASI02) is structurally indistinguishable from legitimate use. The agent invokes the right tools, in the right order, with the right timing---it simply targets unauthorised resources or uses escalated privileges. The \textit{what} and \textit{when} are normal; only the \textit{why} is anomalous.

The boundary is crisp: behavioural anomaly detection catches \textit{how} something is done differently; it cannot catch \textit{why} something is done wrongly. This boundary is identical in both domains, which is itself a finding---it suggests that the structural/semantic distinction is a property of anomaly detection itself, not of any particular application domain.

\subsection{Real-World Validation}

The Anthropic disclosures of February 2026~\cite{anthropic2026distillation} provide unplanned validation of three findings from this paper. First, the GTG-1002 espionage campaign used Claude Code with MCP tools as an autonomous agent---confirming the structural equivalence between insider threat and agent threat patterns that underpins our transfer results (Experiments~1--2, 7). Second, the campaign used task decomposition (splitting espionage into ``defensive testing'' subtasks) as its primary evasion strategy---the same mechanism quantified in Experiment~6, where decomposition costs 5--6\% detection power for IF and DC ($p{=}0.031$). Third, the distillation attacks distributed traffic across 20,000+ accounts so per-account behaviour fell within normal bounds---confirming the HYDRA blind spot (${\sim}$0.54 AUC-ROC) identified in Experiment~5, and Experiment~13's finding that HYDRA sits at the optimal evasion intensity. Anthropic detected these attacks through metadata correlation and network-level analysis, not per-account behavioural monitoring---precisely because per-entity monitoring cannot see coordination patterns. Experiment~9 further quantifies this: mimicry evasion degrades ASI05 detection by 25\% for LSTM.

Experiment~12 adds an important methodological caveat: the ASI02 ``blind spot'' from Experiment~3's synthetic profiling does not replicate on real data (0.86--0.94 AUC-ROC on ATBench vs.\ 0.51--0.52 synthetic). Circular synthetic methodology---where profiles define perturbations and models detect those same perturbations---underestimates real detection performance for categories where real attacks produce richer structural signatures than the synthetic profile assumes.

\subsection{Limitations}

\textbf{UBFS sparsity.} Of 20 UBFS features, 7 show zero variance in both domains (NaN KL divergence in the governance audit). The effective dimensionality is 13. Feature engineering for richer cross-domain overlap is needed.

\textbf{Synthetic OWASP.} The OWASP detection matrix (Experiment~3) uses synthetically injected anomalies with circular methodology. Experiment~12 demonstrates this underestimates real detection: Spearman $\rho$ between synthetic and real rankings is only 0.20. Real-data validation should supplement synthetic profiling in future work.

\textbf{Small agent datasets.} TRAIL (148 samples) and TRACE (517 samples) are orders of magnitude smaller than CMU-CERT (329,000 user-days). Statistical power is limited for agent-domain conclusions.

\textbf{Single schema.} We test one feature schema (UBFS) with one mapping per domain. Alternative schemas or mappings might yield different transfer characteristics.

\textbf{Synthetic insider data.} CMU-CERT is a simulation designed by security experts but does not capture the full diversity of real insider behaviour~\cite{glasser2013bridging}.

\textbf{Semantic feature scope.} The UBFS-28 semantic features (Experiment~8) are computed from sentence-transformer embeddings of tool parameters and conversation context. They capture textual intent signals but not authorisation context, which is the core reason ASI02 remains undetectable.

\subsection{Future Work}

Three directions follow from the experimental results.

\textbf{Authorisation verification alongside better features.} Experiment~8 shows that semantic augmentation fails to close the synthetic ASI02 blind spot, while Experiment~12 shows it improves real-data ASI02 by 4.8--8.1\%. The synthetic blind spot is partially methodological, but the principle stands: intent-based misuse where all parameters are legitimate remains hard to detect. Future work should combine UBFS-28 (effective on real data) with runtime authorisation checks---signed capability grants, per-action permission verification---for the cases where behavioural monitoring still falls short.

\textbf{Coordination-aware detection.} The HYDRA blind spot (Experiment~5) demonstrates that per-entity monitoring cannot detect distributed attacks. Detection requires cross-entity correlation: aggregate volume tracking, account network analysis, and temporal coordination fingerprinting. Anthropic's detection of the distillation attacks relied on exactly these signals~\cite{anthropic2026distillation}. Integrating coordination features into UBFS would address this second structural blind spot.

\textbf{Adversarial-robust features.} Experiment~9 shows that ASI05's high detection rate is brittle---mimicry evasion drops LSTM from 0.976 to 0.731 AUC-ROC. Future work should develop features robust to nearest-neighbour mimicry, such as higher-order interaction terms or temporal signatures that cannot be individually replaced.

\textbf{Adaptive window monitoring.} Experiment~10 reveals that Isolation Forest saturates at 10 spans while LSTM benefits from full traces. A production system should use \textit{adaptive windows}: short windows for fast threshold-based alerts (IF), long windows for temporal pattern detection (LSTM). The 10-span saturation point on ATBench suggests a principled default for MCP-style tool traces.

\textbf{Transformer architectures.} Self-attention~\cite{vaswani2017attention} over behavioural sequences may capture longer-range dependencies than LSTM autoencoders and provide better interpretability through attention weight analysis. The LSTM's robustness to decomposition evasion (Experiment~6, $-0.007$ vs.\ $-0.051$ to $-0.064$ for other models) suggests that sequence-aware architectures are more resistant to temporal evasion strategies---transformers may extend this advantage further.

% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

Seven findings, in order of surprise.

\textbf{First, the transfer works---and generalises.} An Isolation Forest trained on insider threat data retains 97\% of its detection power on agent traces (Experiment~2), and transfer to MCP tool-calling benchmarks exceeds within-domain performance at 104.8\% retention (Experiment~7). The UBFS bridge enables genuine cross-domain anomaly detection across multiple agent benchmark boundaries.

\textbf{Second, there are two structural blind spots, not one.} Tool Misuse (ASI02) is an \textit{intent} blind spot: the agent does the right things for the wrong reasons. Distributed extraction (HYDRA) is a \textit{coordination} blind spot: each entity behaves normally, but the aggregate is anomalous. Both achieve ${\sim}$0.50--0.54 AUC-ROC. Both are fundamental to per-entity behavioural monitoring.

\textbf{Third, synthetic blind spots require real-data validation.} Experiment~3's synthetic profiling identifies ASI02 as a blind spot (${\sim}$0.52 AUC-ROC), and Experiment~8's semantic augmentation leaves it at ${\sim}$0.56. But Experiment~12 reveals this is partially a circular methodology artifact: real ATBench data yields 0.81--0.94 on ASI02, and UBFS-28 adds up to 13\% on real data. The lesson: synthetic profiling provides useful lower bounds but not absolute calibration (Spearman $\rho{=}0.20$ between synthetic and real rankings).

\textbf{Fourth, evasion patterns transfer across domains.} Task decomposition costs 5--6\% detection power for IF and DC ($p{=}0.031$, Experiment~6), while LSTM shows near-invariance ($-0.7\%$, $p{=}0.094$), mirroring the ``boiling frog'' pattern that drops insider threat detection from 69\% to 3\% in~\cite{rimal2026insider}. The Anthropic-disclosed GTG-1002 campaign used exactly this strategy. Distillation sensitivity analysis (Experiment~13) further shows that detection degrades monotonically with attack intensity for FOCUSED/BROAD/COT, but HYDRA sits at an optimal evasion intensity where attenuation paradoxically increases detection.

\textbf{Fifth, high detection rates are brittle.} Experiment~9 shows that mimicry evasion---copying a normal neighbour's features while preserving the attack payload---degrades ASI05 detection by up to 25\%. The categories that \textit{appear} most detectable are the ones most vulnerable to informed adversaries. ASI02, paradoxically, is immune to evasion because it is already at the detection floor. Robustness and detectability are inversely correlated.

\textbf{Sixth, early signals suffice.} Experiment~10 reveals that 10 spans capture the same detection signal as full traces for Isolation Forest on both TRAIL and ATBench. This mirrors the 7-day optimal window from insider detection~\cite{rimal2026insider}. Monitoring systems can use short observation windows without sacrificing accuracy, reducing both computational cost and detection latency.

\textbf{Seventh, the framework generalises beyond OWASP.} All five MITRE ATLAS techniques are strongly detectable through UBFS (${\geq}$0.93 AUC-ROC for IF/LSTM, Experiment~11). No new blind spots emerge. The ATLAS techniques---model replication, extraction, adversarial crafting---have strong volume and frequency signatures that UBFS captures by design. The only blind spot remains intent-based: attacks that use the right tools for the wrong reasons.

The deeper lesson: insider threat detection and AI agent monitoring are not analogous disciplines. They are the \textit{same} discipline, applied to different principals. Building one from scratch while ignoring the other wastes decades of hard-won security knowledge---and risks rediscovering the same blind spots the hard way.

All code, data pipelines, and experimental results are available at \url{https://github.com/BipinRimal314/threat-to-governance-pipeline} to support reproducibility.

% ============================================================================
\section*{Acknowledgments}

The author thanks the CERT Division of the Software Engineering Institute at Carnegie Mellon University for making the CMU-CERT dataset available, and Patronus AI for the TRAIL and TRACE datasets.

% ============================================================================
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
